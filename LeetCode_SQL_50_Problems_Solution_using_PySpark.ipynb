{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b01dae-bda0-4c54-ace9-678709fc7c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SQL 50 (Leetcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc7383e-fbb4-4f5e-a3a4-f6de4960fd59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Recyclable and Low Fat Products\n",
    "####Q1. Write a solution to find the ids of products that are both low fat and recyclable.\n",
    "\n",
    "####Input: \n",
    "Products table:\n",
    "\n",
    "| product_id  | low_fats | recyclable |\n",
    "|-------------|----------|------------|\n",
    "| 0           | Y        | N          |\n",
    "| 1           | Y        | Y          |\n",
    "| 2           | N        | Y          |\n",
    "| 3           | Y        | Y          |\n",
    "| 4           | N        | N          |\n",
    "\n",
    "####Output: \n",
    "| product_id  |\n",
    "|-------------|\n",
    "| 1           |\n",
    "| 3           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d422d04-cddb-4111-bfa1-d962af4ec3de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1
        ],
        [
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# dataframe\n",
    "data = [(0, \"Y\", \"N\"),\n",
    "        (1, \"Y\", \"Y\"),\n",
    "        (2, \"N\", \"Y\"),\n",
    "        (3, \"Y\", \"Y\")]\n",
    "schema = \"product_id int, low_fats string, recyclable string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# Solution\n",
    "result_df = df.where((col(\"low_fats\") == \"Y\") & (col(\"recyclable\") == \"Y\")).select(col(\"product_id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d8090b-dc85-414e-9c9e-ba10b61dbaf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Find Customer Referee\n",
    "####Q2. Find the names of the customer that are not referred by the customer with id = 2.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Customer table:\n",
    "\n",
    "| id | name | referee_id |\n",
    "|----|------|------------|\n",
    "| 1  | Will | null       |\n",
    "| 2  | Jane | null       |\n",
    "| 3  | Alex | 2          |\n",
    "| 4  | Bill | null       |\n",
    "| 5  | Zack | 1          |\n",
    "| 6  | Mark | 2          |\n",
    "\n",
    "####Output: \n",
    "| name |\n",
    "|------|\n",
    "| Will |\n",
    "| Jane |\n",
    "| Bill |\n",
    "| Zack |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c8c2be-61a7-4754-9030-776d6a4f4769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th></tr></thead><tbody><tr><td>Will</td></tr><tr><td>Jane</td></tr><tr><td>Bill</td></tr><tr><td>Zack</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Will"
        ],
        [
         "Jane"
        ],
        [
         "Bill"
        ],
        [
         "Zack"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, lit, coalesce\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# dataframe\n",
    "data = [\n",
    "    (1, \"Will\", None),\n",
    "    (2, \"Jane\", None),\n",
    "    (3, \"Alex\", 2),\n",
    "    (4, \"Bill\", None),\n",
    "    (5, \"Zack\", 1),\n",
    "    (6, \"Mark\", 2)\n",
    "]\n",
    "schema = StructType([StructField(\"id\", IntegerType(), False),\n",
    "                    StructField(\"name\", StringType(), False),\n",
    "                    StructField(\"referee_id\", IntegerType(), True)])\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.withColumn(\"referee_id\", coalesce(col(\"referee_id\"), lit(-1))) \\\n",
    "            .where(col(\"referee_id\") != 2).select(col(\"name\")).distinct()\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77483b2c-b7f3-4168-ab3e-a10bf4758808",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Big Countries\n",
    "\n",
    "A country is big if:\n",
    "\n",
    "- it has an area of at least three million (i.e., 3000000 km2), or\n",
    "\n",
    "- it has a population of at least twenty-five million (i.e., 25000000).\n",
    "\n",
    "####Q3. Write a solution to find the name, population, and area of the big countries. Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "World table:\n",
    "\n",
    "| name        | continent | area    | population | gdp          |\n",
    "|-------------|-----------|---------|------------|--------------|\n",
    "| Afghanistan | Asia      | 652230  | 25500100   | 20343000000  |\n",
    "| Albania     | Europe    | 28748   | 2831741    | 12960000000  |\n",
    "| Algeria     | Africa    | 2381741 | 37100000   | 188681000000 |\n",
    "| Andorra     | Europe    | 468     | 78115      | 3712000000   |\n",
    "| Angola      | Africa    | 1246700 | 20609294   | 100990000000 |\n",
    "\n",
    "####Output: \n",
    "| name        | population | area    |\n",
    "|-------------|------------|---------|\n",
    "| Afghanistan | 25500100   | 652230  |\n",
    "| Algeria     | 37100000   | 2381741 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14df276-a43a-40a5-be3a-ea2a5bd8ed6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>population</th><th>area</th></tr></thead><tbody><tr><td>Afghanistan</td><td>25500100</td><td>652230</td></tr><tr><td>Algeria</td><td>37100000</td><td>2381741</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Afghanistan",
         25500100,
         652230
        ],
        [
         "Algeria",
         37100000,
         2381741
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "population",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "area",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"sparkApp\").getOrCreate()\n",
    "\n",
    "# dataframe\n",
    "data = [\n",
    "        (\"Afghanistan\",\t\"Asia\",\t652230,\t25500100, 20343000000),\n",
    "        (\"Albania\", \"Europe\", 28748, 2831741, 12960000000),\n",
    "        (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000000),\n",
    "        (\"Andorra\", \"Europe\", 468, 78115, 3712000000),\n",
    "        (\"Angola\", \"Africa\", 1246700, 20609294, 100990000000)\n",
    "]\n",
    "schema = \"name string, continent string, area long, population long, gdp long\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.where((col(\"area\") >= 3000000) | (col(\"population\") >= 25000000)) \\\n",
    "            .select(col(\"name\"), col(\"population\"), col(\"area\")).distinct()\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a78dde3-4eb0-4166-9110-b34f21fbb26b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Article Views I\n",
    "\n",
    "####Q.4 Write a solution to find all the authors that viewed at least one of their own articles. Return the result table sorted by id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Views table:\n",
    "\n",
    "| article_id | author_id | viewer_id | view_date  |\n",
    "|------------|-----------|-----------|------------|\n",
    "| 1          | 3         | 5         | 2019-08-01 |\n",
    "| 1          | 3         | 6         | 2019-08-02 |\n",
    "| 2          | 7         | 7         | 2019-08-01 |\n",
    "| 2          | 7         | 6         | 2019-08-02 |\n",
    "| 4          | 7         | 1         | 2019-07-22 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "\n",
    "####Output: \n",
    "| id   |\n",
    "|------|\n",
    "| 4    |\n",
    "| 7    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec7f2bb-79cd-49ed-ba26-81e8b531b280",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>4</td></tr><tr><td>7</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4
        ],
        [
         7
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports \n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# dataframe\n",
    "data = [\n",
    "    (1, 3, 5, '2019-08-01'),\n",
    "    (1, 3, 6, '2019-08-02'),\n",
    "    (2, 7, 7, '2019-08-01'),\n",
    "    (2, 7, 6, '2019-08-02'),\n",
    "    (4, 7, 1, '2019-07-22'),\n",
    "    (3, 4, 4, '2019-07-21'),\n",
    "    (3, 4, 4, '2019-07-21')\n",
    "]\n",
    "schema = \"article_id int, author_id int, viewer_id int, view_date string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df = df.withColumn(\"view_date\", to_date(col(\"view_date\"), 'yyyy-MM-dd'))\n",
    "\n",
    "# solution\n",
    "result_df = df.filter(col(\"author_id\") == col(\"viewer_id\")) \\\n",
    "    .select(col(\"author_id\").alias(\"id\")).distinct().orderBy(\"id\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44bb3be4-becf-4fcd-9a74-c2e2b49ecdd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Invalid Tweets\n",
    "\n",
    "####Q.5 Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Tweets table:\n",
    "\n",
    "| tweet_id | content                          |\n",
    "|----------|----------------------------------|\n",
    "| 1        | Vote for Biden                   |\n",
    "| 2        | Let us make America great again! |\n",
    "\n",
    "####Output: \n",
    "| tweet_id |\n",
    "|----------|\n",
    "| 2        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff05bcb-d1d7-4ed0-bd94-3093bf34cb4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>tweet_id</th></tr></thead><tbody><tr><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "tweet_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [(1, 'Vote for Biden'),\n",
    "        (2, 'Let us make America great again!')]\n",
    "schema = \"tweet_id integer, content string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.filter(length(col(\"content\")) > 15).select(col(\"tweet_id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de9b59b-2c3d-4fd9-9b69-f9a2e5f41d28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Replace Employee ID With The Unique Identifier\n",
    "####Q.6 Write a solution to show the unique ID of each user, If a user does not have a unique ID replace just show null.Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Employees table:\n",
    "\n",
    "| id | name     |\n",
    "|----|----------|\n",
    "| 1  | Alice    |\n",
    "| 7  | Bob      |\n",
    "| 11 | Meir     |\n",
    "| 90 | Winston  |\n",
    "| 3  | Jonathan |\n",
    "\n",
    "EmployeeUNI table:\n",
    "\n",
    "| id | unique_id |\n",
    "|----|-----------|\n",
    "| 3  | 1         |\n",
    "| 11 | 2         |\n",
    "| 90 | 3         |\n",
    "\n",
    "####Output: \n",
    "| unique_id | name     |\n",
    "|-----------|----------|\n",
    "| null      | Alice    |\n",
    "| null      | Bob      |\n",
    "| 2         | Meir     |\n",
    "| 3         | Winston  |\n",
    "| 1         | Jonathan |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fce51f0-5646-4beb-899e-bc6ed49c815e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>unique_id</th><th>name</th></tr></thead><tbody><tr><td>null</td><td>Alice</td></tr><tr><td>null</td><td>Bob</td></tr><tr><td>2</td><td>Meir</td></tr><tr><td>3</td><td>Winston</td></tr><tr><td>1</td><td>Jonathan</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         "Alice"
        ],
        [
         null,
         "Bob"
        ],
        [
         2,
         "Meir"
        ],
        [
         3,
         "Winston"
        ],
        [
         1,
         "Jonathan"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "unique_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "# spark sesion\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [(1, \"Alice\"),\n",
    "         (7, \"Bob\"),\n",
    "         (11, \"Meir\"),\n",
    "         (90, \"Winston\"),\n",
    "         (3, \"Jonathan\")]\n",
    "schema1 = \"id integer, name string\"\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [(3, 1),\n",
    "         (11, 2),\n",
    "         (90, 3)]\n",
    "schema2 = \"id integer, unique_id integer\"\n",
    "df2 = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "joined_df = df1.join(df2, df1.id == df2.id, \"left_outer\")\n",
    "result_df = joined_df.select(col(\"unique_id\"), col(\"name\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533eb8f5-e8b7-4b38-8f97-cb2074a1514c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Product Sales Analysis I\n",
    "####Q.7 Write a solution to report the product_name, year, and price for each sale_id in the Sales table. Return the resulting table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Sales table:\n",
    "\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "|---------|------------|------|----------|-------|\n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "\n",
    "Product table:\n",
    "\n",
    "| product_id | product_name |\n",
    "|------------|--------------|\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "\n",
    "####Output: \n",
    "| product_name | year  | price |\n",
    "|--------------|-------|-------|\n",
    "| Nokia        | 2008  | 5000  |\n",
    "| Nokia        | 2009  | 5000  |\n",
    "| Apple        | 2011  | 9000  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378a2d9c-abbc-4fb7-9b77-e9c3b01fdc30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_name</th><th>year</th><th>price</th></tr></thead><tbody><tr><td>Nokia</td><td>2008</td><td>5000</td></tr><tr><td>Nokia</td><td>2009</td><td>5000</td></tr><tr><td>Apple</td><td>2011</td><td>9000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Nokia",
         2008,
         5000
        ],
        [
         "Nokia",
         2009,
         5000
        ],
        [
         "Apple",
         2011,
         9000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe1\n",
    "data1 =  [(1, 100, 2008, 10, 5000),\n",
    "          (2, 100, 2009, 12, 5000),\n",
    "          (7, 200, 2011, 15, 9000)]\n",
    "\n",
    "schema1 = \"sale_id int, product_id int, year int, quantity int, price int\"\n",
    "\n",
    "df_sales = spark.createDataFrame(data=data1, schema=schema1)\n",
    "\n",
    "# input dataframe2\n",
    "data2 =  [(100, \"Nokia\"),\n",
    "          (200, \"Apple\"),\n",
    "          (300, \"Samsung\")]\n",
    "\n",
    "schema2 = \"product_id int, product_name string\"\n",
    "\n",
    "df_product = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "joined_df = df_sales.join(df_product, df_sales.product_id == df_product.product_id, \"inner\")\n",
    "result_df = joined_df.select(col(\"product_name\"), col(\"year\"), col(\"price\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c52834-e0f6-4c0c-9b37-0120c8d3c84a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Customer Who Visited but Did Not Make Any Transactions\n",
    "####Q.8 Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits. Return the result table sorted in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input:\n",
    "Visits\n",
    "\n",
    "| visit_id | customer_id |\n",
    "|----------|-------------|\n",
    "| 1        | 23          |\n",
    "| 2        | 9           |\n",
    "| 4        | 30          |\n",
    "| 5        | 54          |\n",
    "| 6        | 96          |\n",
    "| 7        | 54          |\n",
    "| 8        | 54          |\n",
    "\n",
    "Transactions\n",
    "\n",
    "| transaction_id | visit_id | amount |\n",
    "|----------------|----------|--------|\n",
    "| 2              | 5        | 310    |\n",
    "| 3              | 5        | 300    |\n",
    "| 9              | 5        | 200    |\n",
    "| 12             | 1        | 910    |\n",
    "| 13             | 2        | 970    |\n",
    "\n",
    "####Output: \n",
    "| customer_id | count_no_trans |\n",
    "|-------------|----------------|\n",
    "| 54          | 2              |\n",
    "| 30          | 1              |\n",
    "| 96          | 1              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f037b1e8-5379-47ef-a9dd-242cf22d650a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>count_no_trans</th></tr></thead><tbody><tr><td>54</td><td>2</td></tr><tr><td>96</td><td>1</td></tr><tr><td>30</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         54,
         2
        ],
        [
         96,
         1
        ],
        [
         30,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count_no_trans",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [(1, 23),\n",
    "         (2, 9),\n",
    "         (4, 30),\n",
    "         (5, 54),\n",
    "         (6, 96),\n",
    "         (7, 54),\n",
    "         (8, 54)]\n",
    "schema1 = \"visit_id int, customer_id int\"\n",
    "Visits_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [(2, 5, 310),\n",
    "         (3, 5, 300),   \n",
    "         (9, 5, 200),\n",
    "         (12, 1, 910),\n",
    "         (13, 2, 970)]\n",
    "schema2 = \"transaction_id int, visit_id int, amount int\"\n",
    "transactions_df = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "joined_df = Visits_df.join(transactions_df, Visits_df.visit_id == transactions_df.visit_id, \"left_outer\")\n",
    "result_df = joined_df.filter(col(\"transaction_id\").isNull()).groupBy(col(\"customer_id\")) \\\n",
    "            .agg(count(\"*\").alias(\"count_no_trans\")).sort(col(\"count_no_trans\").desc())\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f75bcf8-e30e-423a-a35c-40cac19f9015",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Rising Temperature\n",
    "####Q.9 Write a solution to find all dates' Id with higher temperatures compared to its previous dates (yesterday). Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Weather table:\n",
    "\n",
    "| id | recordDate | temperature |\n",
    "|----|------------|-------------|\n",
    "| 1  | 2015-01-01 | 10          |\n",
    "| 2  | 2015-01-02 | 25          |\n",
    "| 3  | 2015-01-03 | 20          |\n",
    "| 4  | 2015-01-04 | 30          |\n",
    "\n",
    "####Output: \n",
    "| id |\n",
    "|----|\n",
    "| 2  |\n",
    "| 4  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92830201-67b5-4f0c-9d55-6f8ffa4f7422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>2</td></tr><tr><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ],
        [
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [(1, '2015-01-01', 10),\n",
    "        (2, '2015-01-02', 25),        \n",
    "        (3, '2015-01-03', 20),          \n",
    "        (4, '2015-01-04', 30)]\n",
    "schema = [\"id\", \"recordDate\", \"temperature\"]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df = df.withColumn(\"recordDate\", to_date(col(\"recordDate\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# solution\n",
    "window_spec = Window.orderBy(\"id\")\n",
    "result_df = df.withColumn(\"previous_temperature\", lag(\"temperature\", 1).over(window_spec)) \\\n",
    "            .filter(col(\"temperature\") > col(\"previous_temperature\")).select(col(\"id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639389c3-fd91-4ef5-8aa8-4418ad9ab793",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Average Time of Process per Machine\n",
    "####Q.10 There is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process. The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run. The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Activity table:\n",
    "\n",
    "| machine_id | process_id | activity_type | timestamp |\n",
    "|------------|------------|---------------|-----------|\n",
    "| 0          | 0          | start         | 0.712     |\n",
    "| 0          | 0          | end           | 1.520     |\n",
    "| 0          | 1          | start         | 3.140     |\n",
    "| 0          | 1          | end           | 4.120     |\n",
    "| 1          | 0          | start         | 0.550     |\n",
    "| 1          | 0          | end           | 1.550     |\n",
    "| 1          | 1          | start         | 0.430     |\n",
    "| 1          | 1          | end           | 1.420     |\n",
    "| 2          | 0          | start         | 4.100     |\n",
    "| 2          | 0          | end           | 4.512     |\n",
    "| 2          | 1          | start         | 2.500     |\n",
    "| 2          | 1          | end           | 5.000     |\n",
    "\n",
    "####Output: \n",
    "| machine_id | processing_time |\n",
    "|------------|-----------------|\n",
    "| 0          | 0.894           |\n",
    "| 1          | 0.995           |\n",
    "| 2          | 1.456           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a04e1ca-344a-4b84-8d9f-4a06f6247d58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>machine_id</th><th>processing_time</th></tr></thead><tbody><tr><td>0</td><td>0.894</td></tr><tr><td>1</td><td>0.995</td></tr><tr><td>2</td><td>1.456</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         0.894
        ],
        [
         1,
         0.995
        ],
        [
         2,
         1.456
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "machine_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "processing_time",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lead, avg, round\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (0, 0, 'start', 0.712),\n",
    "    (0, 0, 'end', 1.520),\n",
    "    (0, 1, 'start', 3.140),\n",
    "    (0, 1, 'end', 4.120),\n",
    "    (1, 0, 'start', 0.550),\n",
    "    (1, 0, 'end', 1.550),\n",
    "    (1, 1, 'start', 0.430),\n",
    "    (1, 1, 'end', 1.420),\n",
    "    (2, 0, 'start', 4.100),\n",
    "    (2, 0, 'end', 4.512),\n",
    "    (2, 1, 'start', 2.500),\n",
    "    (2, 1, 'end', 5.000)\n",
    "]\n",
    "schema = \"machine_id int, process_id int, activity_type string, timestamp float\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "#solution\n",
    "window_spec = Window.partitionBy(col(\"machine_id\"), col(\"process_id\")).orderBy(col(\"timestamp\"))\n",
    "lead_df = df.withColumn(\"lead_timestamp\", lead(col(\"timestamp\"), 1).over(window_spec)) \\\n",
    "         .filter(col(\"activity_type\") == 'start')\n",
    "diff_df = lead_df.withColumn(\"timestamp_diff\", col(\"lead_timestamp\")-col(\"timestamp\"))\n",
    "avg_df = diff_df.groupBy(col(\"machine_id\")).agg(avg(col(\"timestamp_diff\")).alias(\"processing_time\"))\n",
    "result_df = avg_df.withColumn(\"processing_time\", round(col(\"processing_time\"), 3)).sort(col(\"processing_time\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f358c30-6394-4bc5-a630-f27e5084790a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Employee Bonus\n",
    "####Q.11 Write a solution to report the name and bonus amount of each employee with a bonus less than 1000.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Employee table:\n",
    "\n",
    "| empId | name   | supervisor | salary |\n",
    "|-------|--------|------------|--------|\n",
    "| 3     | Brad   | null       | 4000   |\n",
    "| 1     | John   | 3          | 1000   |\n",
    "| 2     | Dan    | 3          | 2000   |\n",
    "| 4     | Thomas | 3          | 4000   |\n",
    "\n",
    "Bonus table:\n",
    "\n",
    "| empId | bonus |\n",
    "|-------|-------|\n",
    "| 2     | 500   |\n",
    "| 4     | 2000  |\n",
    "\n",
    "####Output: \n",
    "| name | bonus |\n",
    "|------|-------|\n",
    "| Brad | null  |\n",
    "| John | null  |\n",
    "| Dan  | 500   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c747468-01af-4a87-affc-3e0a6af513e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>bonus</th></tr></thead><tbody><tr><td>Brad</td><td>null</td></tr><tr><td>John</td><td>null</td></tr><tr><td>Dan</td><td>500</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Brad",
         null
        ],
        [
         "John",
         null
        ],
        [
         "Dan",
         500
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [\n",
    "    (3, 'Brad', None, 4000),\n",
    "    (1, 'John', 3, 1000),\n",
    "    (2, 'Dan', 3, 2000),\n",
    "    (4, 'Thomas', 3, 4000)\n",
    "]\n",
    "schema1 = \"empId int, name string, supervisor int, salary int\"\n",
    "employee_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (2, 500),\n",
    "    (4, 2000)\n",
    "]\n",
    "schema2 = \"empId int, bonus int\"\n",
    "bonus_df = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "joined_df = employee_df.join(bonus_df, employee_df.empId == bonus_df.empId, \"left_outer\")\n",
    "result_df = joined_df.filter((col(\"bonus\") < 1000) | (col(\"bonus\").isNull())).select(col(\"name\"), col(\"bonus\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692de79b-0d8f-475a-8565-1ffa27f046eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Students and Examinations\n",
    "####Q.12 Write a solution to find the number of times each student attended each exam.\n",
    "\n",
    "Return the result table ordered by student_id and subject_name.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Students table:\n",
    "\n",
    "| student_id | student_name |\n",
    "|------------|--------------|\n",
    "| 1          | Alice        |\n",
    "| 2          | Bob          |\n",
    "| 13         | John         |\n",
    "| 6          | Alex         |\n",
    "\n",
    "Subjects table:\n",
    "\n",
    "| subject_name |\n",
    "|--------------|\n",
    "| Math         |\n",
    "| Physics      |\n",
    "| Programming  |\n",
    "\n",
    "Examinations table:\n",
    "\n",
    "| student_id | subject_name |\n",
    "|------------|--------------|\n",
    "| 1          | Math         |\n",
    "| 1          | Physics      |\n",
    "| 1          | Programming  |\n",
    "| 2          | Programming  |\n",
    "| 1          | Physics      |\n",
    "| 1          | Math         |\n",
    "| 13         | Math         |\n",
    "| 13         | Programming  |\n",
    "| 13         | Physics      |\n",
    "| 2          | Math         |\n",
    "| 1          | Math         |\n",
    "\n",
    "####Output: \n",
    "| student_id | student_name | subject_name | attended_exams |\n",
    "|------------|--------------|--------------|----------------|\n",
    "| 1          | Alice        | Math         | 3              |\n",
    "| 1          | Alice        | Physics      | 2              |\n",
    "| 1          | Alice        | Programming  | 1              |\n",
    "| 2          | Bob          | Math         | 1              |\n",
    "| 2          | Bob          | Physics      | 0              |\n",
    "| 2          | Bob          | Programming  | 1              |\n",
    "| 6          | Alex         | Math         | 0              |\n",
    "| 6          | Alex         | Physics      | 0              |\n",
    "| 6          | Alex         | Programming  | 0              |\n",
    "| 13         | John         | Math         | 1              |\n",
    "| 13         | John         | Physics      | 1              |\n",
    "| 13         | John         | Programming  | 1              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aea719d-605e-4932-8af8-439be979ff57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>student_id</th><th>student_name</th><th>subject_name</th><th>attended_exams</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>Math</td><td>3</td></tr><tr><td>1</td><td>Alice</td><td>Physics</td><td>2</td></tr><tr><td>1</td><td>Alice</td><td>Programming</td><td>1</td></tr><tr><td>2</td><td>Bob</td><td>Math</td><td>1</td></tr><tr><td>2</td><td>Bob</td><td>Physics</td><td>0</td></tr><tr><td>2</td><td>Bob</td><td>Programming</td><td>1</td></tr><tr><td>6</td><td>Alex</td><td>Math</td><td>0</td></tr><tr><td>6</td><td>Alex</td><td>Physics</td><td>0</td></tr><tr><td>6</td><td>Alex</td><td>Programming</td><td>0</td></tr><tr><td>13</td><td>John</td><td>Math</td><td>1</td></tr><tr><td>13</td><td>John</td><td>Physics</td><td>1</td></tr><tr><td>13</td><td>John</td><td>Programming</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         "Math",
         3
        ],
        [
         1,
         "Alice",
         "Physics",
         2
        ],
        [
         1,
         "Alice",
         "Programming",
         1
        ],
        [
         2,
         "Bob",
         "Math",
         1
        ],
        [
         2,
         "Bob",
         "Physics",
         0
        ],
        [
         2,
         "Bob",
         "Programming",
         1
        ],
        [
         6,
         "Alex",
         "Math",
         0
        ],
        [
         6,
         "Alex",
         "Physics",
         0
        ],
        [
         6,
         "Alex",
         "Programming",
         0
        ],
        [
         13,
         "John",
         "Math",
         1
        ],
        [
         13,
         "John",
         "Physics",
         1
        ],
        [
         13,
         "John",
         "Programming",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "student_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "student_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subject_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attended_exams",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataFrame\n",
    "data1 = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (13, \"John\"),\n",
    "    (6, \"Alex\")\n",
    "]\n",
    "schema1 = \"student_id int, student_name string\"\n",
    "student_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (\"Math\",),\n",
    "    (\"Physics\",),\n",
    "    (\"Programming\",)\n",
    "]\n",
    "schema2 = \"subject_name string\"\n",
    "subject_df = spark.createDataFrame(data=data2, schema=schema2)\n",
    "data3 = [\n",
    "    (1, \"Math\"),\n",
    "    (1, \"Physics\"),\n",
    "    (1, \"Programming\"),\n",
    "    (2, \"Programming\"),\n",
    "    (1, \"Physics\"),\n",
    "    (1, \"Math\"),\n",
    "    (13, \"Math\"),\n",
    "    (13, \"Programming\"),\n",
    "    (13, \"Physics\"),\n",
    "    (2, \"Math\"),\n",
    "    (1, \"Math\")\n",
    "]\n",
    "schema3 = \"student_id int, subject_name string\"\n",
    "examination_df = spark.createDataFrame(data=data3, schema=schema3)\n",
    "\n",
    "# solution\n",
    "student_subject_df = student_df.crossJoin(subject_df)\n",
    "count_df = examination_df.groupBy(col(\"student_id\"), col(\"subject_name\")).agg(count(\"*\").alias(\"attended_exams\"))\n",
    "result_df = student_subject_df.join(count_df, (student_subject_df.student_id == count_df.student_id) & \\\n",
    "            (student_subject_df.subject_name == count_df.subject_name), \"left_outer\") \\\n",
    "            .select(student_subject_df.student_id, col(\"student_name\"), student_subject_df.subject_name, \\\n",
    "             col(\"attended_exams\")).fillna(0, subset=[\"attended_exams\"])\n",
    "result_df = result_df.orderBy(col(\"student_id\"), col(\"subject_name\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222afe4b-f2c1-4b90-bfa2-545fd6c9f20e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Managers with at Least 5 Direct Reports\n",
    "####Q.13 Write a solution to find managers with at least five direct reports.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Employee table:\n",
    "\n",
    "| id  | name  | department | managerId |\n",
    "|-----|-------|------------|-----------|\n",
    "| 101 | John  | A          | null      |\n",
    "| 102 | Dan   | A          | 101       |\n",
    "| 103 | James | A          | 101       |\n",
    "| 104 | Amy   | A          | 101       |\n",
    "| 105 | Anne  | A          | 101       |\n",
    "| 106 | Ron   | B          | 101       |\n",
    "\n",
    "####Output: \n",
    "| name |\n",
    "|------|\n",
    "| John |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2a9361-cb47-45f5-83b3-4a1d514db862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th></tr></thead><tbody><tr><td>John</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (101, \"John\", \"A\", None),\n",
    "    (102, \"Dan\", \"A\", 101),\n",
    "    (103, \"James\", \"A\", 101),\n",
    "    (104, \"Amy\", \"A\", 101),\n",
    "    (105, \"Anne\", \"A\", 101),\n",
    "    (106, \"Ron\", \"B\", 101)\n",
    "]\n",
    "schema = \"id int, name string, department string, managerId int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df1 = df.alias(\"df1\")\n",
    "df2 = df.alias(\"df2\")\n",
    "\n",
    "# solution\n",
    "count_df = df1.groupBy(col(\"df1.managerId\")).agg(count(\"*\").alias(\"reporting_count\")) \\\n",
    "          .filter(col(\"reporting_count\") >= 5)\n",
    "result_df = count_df.join(df2, col(\"df1.managerId\") == col(\"df2.id\"), \"inner\").select(col(\"df2.name\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8bc321-c6ec-490b-b006-c22ba38b04ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Confirmation Rate\n",
    "####Q.14 The confirmation rate of a user is the number of 'confirmed' messages divided by the total number of requested confirmation messages. The confirmation rate of a user that did not request any confirmation messages is 0. Round the confirmation rate to two decimal places. Write a solution to find the confirmation rate of each user.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Signups table:\n",
    "\n",
    "| user_id | time_stamp          |\n",
    "|---------|---------------------|\n",
    "| 3       | 2020-03-21 10:16:13 |\n",
    "| 7       | 2020-01-04 13:57:59 |\n",
    "| 2       | 2020-07-29 23:09:44 |\n",
    "| 6       | 2020-12-09 10:39:37 |\n",
    "\n",
    "Confirmations table:\n",
    "\n",
    "| user_id | time_stamp          | action    |\n",
    "|---------|---------------------|-----------|\n",
    "| 3       | 2021-01-06 03:30:46 | timeout   |\n",
    "| 3       | 2021-07-14 14:00:00 | timeout   |\n",
    "| 7       | 2021-06-12 11:57:29 | confirmed |\n",
    "| 7       | 2021-06-13 12:58:28 | confirmed |\n",
    "| 7       | 2021-06-14 13:59:27 | confirmed |\n",
    "| 2       | 2021-01-22 00:00:00 | confirmed |\n",
    "| 2       | 2021-02-28 23:59:59 | timeout   |\n",
    "\n",
    "####Output: \n",
    "| user_id | confirmation_rate |\n",
    "|---------|-------------------|\n",
    "| 6       | 0.00              |\n",
    "| 3       | 0.00              |\n",
    "| 7       | 1.00              |\n",
    "| 2       | 0.50              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a96aae1e-416d-44ea-9a25-96d34dcb79f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>confirmation_rate</th></tr></thead><tbody><tr><td>3</td><td>0.0</td></tr><tr><td>6</td><td>0.0</td></tr><tr><td>2</td><td>0.5</td></tr><tr><td>7</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         0.0
        ],
        [
         6,
         0.0
        ],
        [
         2,
         0.5
        ],
        [
         7,
         1.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "confirmation_rate",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, round\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [\n",
    "    (3, \"2020-03-21 10:16:13\"),\n",
    "    (7, \"2020-01-04 13:57:59\"),\n",
    "    (2, \"2020-07-29 23:09:44\"),\n",
    "    (6, \"2020-12-09 10:39:37\")\n",
    "]\n",
    "schema1 = \"user_id int, time_stamp string\"\n",
    "signups_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (3, \"2021-01-06 03:30:46\", \"timeout\"),\n",
    "    (3, \"2021-07-14 14:00:00\", \"timeout\"),\n",
    "    (7, \"2021-06-12 11:57:29\", \"confirmed\"),\n",
    "    (7, \"2021-06-13 12:58:28\", \"confirmed\"),\n",
    "    (7, \"2021-06-14 13:59:27\", \"confirmed\"),\n",
    "    (2, \"2021-01-22 00:00:00\", \"confirmed\"),\n",
    "    (2, \"2021-02-28 23:59:59\", \"timeout\")\n",
    "]\n",
    "schema2 = \"user_id int, time_stamp string, action string\"\n",
    "confirmations_df = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "aggregated_df = confirmations_df.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_actions\"),\n",
    "        count(when(col(\"action\") == \"confirmed\", True)).alias(\"confirmed_actions\")\n",
    "    )\n",
    "result_df = aggregated_df.withColumn(\n",
    "    \"confirmation_rate\", \n",
    "    round(col(\"confirmed_actions\") / col(\"total_actions\"), 2)\n",
    ")\n",
    "final_df = signups_df.join(result_df, \"user_id\", \"left\").select(\n",
    "    col(\"user_id\"),\n",
    "    col(\"confirmation_rate\")\n",
    ").fillna({\"confirmation_rate\": 0.00}).sort(col(\"confirmation_rate\"))\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6ed4b0-fdce-4446-8737-39b62c49298a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Not Boring Movies\n",
    "####Q.15 Write a solution to report the movies with an odd-numbered ID and a description that is not \"boring\".\n",
    "\n",
    "Return the result table ordered by rating in descending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Cinema table:\n",
    "\n",
    "| id | movie      | description | rating |\n",
    "|----|------------|-------------|--------|\n",
    "| 1  | War        | great 3D    | 8.9    |\n",
    "| 2  | Science    | fiction     | 8.5    |\n",
    "| 3  | irish      | boring      | 6.2    |\n",
    "| 4  | Ice song   | Fantacy     | 8.6    |\n",
    "| 5  | House card | Interesting | 9.1    |\n",
    "\n",
    "####Output: \n",
    "| id | movie      | description | rating |\n",
    "|----|------------|-------------|--------|\n",
    "| 5  | House card | Interesting | 9.1    |\n",
    "| 1  | War        | great 3D    | 8.9    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a097eb-9859-4c9b-b626-756614d5a881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>movie</th><th>description</th><th>rating</th></tr></thead><tbody><tr><td>5</td><td>House card</td><td>Interesting</td><td>9.1</td></tr><tr><td>1</td><td>War</td><td>great 3D</td><td>8.9</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5,
         "House card",
         "Interesting",
         9.1
        ],
        [
         1,
         "War",
         "great 3D",
         8.9
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "movie",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, \"War\", \"great 3D\", 8.9),\n",
    "    (2, \"Science\", \"fiction\", 8.5),\n",
    "    (3, \"irish\", \"boring\", 6.2),\n",
    "    (4, \"Ice song\", \"Fantasy\", 8.6),\n",
    "    (5, \"House card\", \"Interesting\", 9.1)\n",
    "]\n",
    "schema = \"id int, movie string, description string, rating float\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.filter((col(\"id\")%2!=0) & (col(\"description\")!=\"boring\")).sort(col(\"rating\").desc()).select(\"*\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96066c4c-52f8-4cd9-8e93-b62c40b508b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Average Selling Price\n",
    "####Q.16 Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Prices table:\n",
    "\n",
    "| product_id | start_date | end_date   | price  |\n",
    "|------------|------------|------------|--------|\n",
    "| 1          | 2019-02-17 | 2019-02-28 | 5      |\n",
    "| 1          | 2019-03-01 | 2019-03-22 | 20     |\n",
    "| 2          | 2019-02-01 | 2019-02-20 | 15     |\n",
    "| 2          | 2019-02-21 | 2019-03-31 | 30     |\n",
    "\n",
    "UnitsSold table:\n",
    "\n",
    "| product_id | purchase_date | units |\n",
    "|------------|---------------|-------|\n",
    "| 1          | 2019-02-25    | 100   |\n",
    "| 1          | 2019-03-01    | 15    |\n",
    "| 2          | 2019-02-10    | 200   |\n",
    "| 2          | 2019-03-22    | 30    |\n",
    "\n",
    "####Output: \n",
    "| product_id | average_price |\n",
    "|------------|---------------|\n",
    "| 1          | 6.96          |\n",
    "| 2          | 16.96         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a282d5cf-6a0a-4203-ac7c-f00d3fd86220",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>average_price</th></tr></thead><tbody><tr><td>1</td><td>6.96</td></tr><tr><td>2</td><td>16.96</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         6.96
        ],
        [
         2,
         16.96
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "average_price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, round\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [\n",
    "    (1, \"2019-02-17\", \"2019-02-28\", 5),\n",
    "    (1, \"2019-03-01\", \"2019-03-22\", 20),\n",
    "    (2, \"2019-02-01\", \"2019-02-20\", 15),\n",
    "    (2, \"2019-02-21\", \"2019-03-31\", 30)\n",
    "]\n",
    "schema1 = \"product_id int, start_date string, end_date string, price int\"\n",
    "prices_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (1, \"2019-02-25\", 100),\n",
    "    (1, \"2019-03-01\", 15),\n",
    "    (2, \"2019-02-10\", 200),\n",
    "    (2, \"2019-03-22\", 30)\n",
    "]\n",
    "schema2 = \"product_id int, purchase_date string, units int\"\n",
    "unitsold_df = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# Solution\n",
    "joined_df = prices_df.join(unitsold_df, \n",
    "                           (prices_df.product_id == unitsold_df.product_id) & \n",
    "                           (unitsold_df.purchase_date >= prices_df.start_date) & \n",
    "                           (unitsold_df.purchase_date <= prices_df.end_date),\n",
    "                           \"inner\") \\\n",
    "            .select(prices_df.product_id, col(\"price\"), col(\"units\"))\n",
    "result_df = joined_df.groupBy(col(\"product_id\")) \\\n",
    "    .agg(sum(col(\"price\") * col(\"units\")).alias(\"total_revenue\"),\n",
    "         sum(col(\"units\")).alias(\"total_units\")) \\\n",
    "    .select(col(\"product_id\"), \n",
    "            round(col(\"total_revenue\") / col(\"total_units\"), 2).alias(\"average_price\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4434c075-2814-4d09-a3cc-7ea3354d1f31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Project Employees I\n",
    "####Q.17 Write an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Project table:\n",
    "\n",
    "| project_id  | employee_id |\n",
    "|-------------|-------------|\n",
    "| 1           | 1           |\n",
    "| 1           | 2           |\n",
    "| 1           | 3           |\n",
    "| 2           | 1           |\n",
    "| 2           | 4           |\n",
    "\n",
    "Employee table:\n",
    "\n",
    "| employee_id | name   | experience_years |\n",
    "-------------|--------|------------------|\n",
    "| 1           | Khaled | 3                |\n",
    "| 2           | Ali    | 2                |\n",
    "| 3           | John   | 1                |\n",
    "| 4           | Doe    | 2                |\n",
    "\n",
    "####Output: \n",
    "| project_id  | average_years |\n",
    "|-------------|---------------|\n",
    "| 1           | 2.00          |\n",
    "| 2           | 2.50          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b6fdcd-3e00-4e8f-b4db-45b8b113ee53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>project_id</th><th>average_years</th></tr></thead><tbody><tr><td>1</td><td>2.00</td></tr><tr><td>2</td><td>2.50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2.00"
        ],
        [
         2,
         "2.50"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "project_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "average_years",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, format_number\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [\n",
    "    (1, 1),\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "    (2, 1),\n",
    "    (2, 4)\n",
    "]\n",
    "schema1 = \"project_id int, employee_id int\"\n",
    "project_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (1, \"Khaled\", 3),\n",
    "    (2, \"Ali\", 2),\n",
    "    (3, \"John\", 1),\n",
    "    (4, \"Doe\", 2)\n",
    "]\n",
    "schema2 = \"employee_id int, name string, experience_years int\"\n",
    "employee_df = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "joined_df = project_df.join(employee_df, project_df.employee_id == employee_df.employee_id, \"inner\") \\\n",
    "            .select(col(\"project_id\"), project_df.employee_id.alias(\"employee_id\"), col(\"experience_years\"))\n",
    "result_df = joined_df.groupBy(col(\"project_id\")) \\\n",
    "            .agg(\n",
    "                count(\"employee_id\").alias(\"emp_count\"),\n",
    "                sum(\"experience_years\").alias(\"total_experience\")\n",
    "            ) \\\n",
    "            .select(\n",
    "                col(\"project_id\"),\n",
    "                format_number(col(\"total_experience\") / col(\"emp_count\"), 2).alias(\"average_years\")\n",
    "            )\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd78dd32-3c92-40b4-9156-71385ddddff1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Percentage of Users Attended a Contest\n",
    "####Q.18 Write a solution to find the percentage of the users registered in each contest rounded to two decimals.\n",
    "\n",
    "Return the result table ordered by percentage in descending order. In case of a tie, order it by contest_id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Users table:\n",
    "\n",
    "| user_id | user_name |\n",
    "|---------|-----------|\n",
    "| 6       | Alice     |\n",
    "| 2       | Bob       |\n",
    "| 7       | Alex      |\n",
    "\n",
    "Register table:\n",
    "\n",
    "| contest_id | user_id |\n",
    "|------------|---------|\n",
    "| 215        | 6       |\n",
    "| 209        | 2       |\n",
    "| 208        | 2       |\n",
    "| 210        | 6       |\n",
    "| 208        | 6       |\n",
    "| 209        | 7       |\n",
    "| 209        | 6       |\n",
    "| 215        | 7       |\n",
    "| 208        | 7       |\n",
    "| 210        | 2       |\n",
    "| 207        | 2       |\n",
    "| 210        | 7       |\n",
    "\n",
    "####Output: \n",
    "| contest_id | percentage |\n",
    "|------------|------------|\n",
    "| 208        | 100.0      |\n",
    "| 209        | 100.0      |\n",
    "| 210        | 100.0      |\n",
    "| 215        | 66.67      |\n",
    "| 207        | 33.33      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2392b0-340b-4ab5-96d3-1034dde95e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>contest_id</th><th>percentage</th></tr></thead><tbody><tr><td>208</td><td>100.0</td></tr><tr><td>209</td><td>100.0</td></tr><tr><td>210</td><td>100.0</td></tr><tr><td>215</td><td>66.67</td></tr><tr><td>207</td><td>33.33</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         208,
         100.0
        ],
        [
         209,
         100.0
        ],
        [
         210,
         100.0
        ],
        [
         215,
         66.67
        ],
        [
         207,
         33.33
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "contest_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "percentage",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, asc, round\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [\n",
    "    (6, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (7, \"Alex\")\n",
    "]\n",
    "schema1 = \"user_id int, user_name string\"\n",
    "users_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (215, 6),\n",
    "    (209, 2),\n",
    "    (208, 2),\n",
    "    (210, 6),\n",
    "    (208, 6),\n",
    "    (209, 7),\n",
    "    (209, 6),\n",
    "    (215, 7),\n",
    "    (208, 7),\n",
    "    (210, 2),\n",
    "    (207, 2),\n",
    "    (210, 7)\n",
    "]\n",
    "schema2 = \"contest_id int, user_id int\"\n",
    "register_df = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "total_user_count = users_df.count()\n",
    "result_df = register_df.groupBy(col(\"contest_id\")) \\\n",
    "            .agg(count(col(\"user_id\")).alias(\"participated_user_count\")) \\\n",
    "            .select(col(\"contest_id\"), \n",
    "                    round((col(\"participated_user_count\")/total_user_count)*100, 2).alias(\"percentage\")) \\\n",
    "            .orderBy(col(\"percentage\").desc(), col(\"contest_id\").asc())\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836ee1ca-f34a-49b1-88b7-4e7c58e863aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Queries Quality and Percentage\n",
    "We define query quality as: \n",
    "The average of the ratio between query rating and its position.\n",
    "We also define poor query percentage as:\n",
    "The percentage of all queries with rating less than 3.\n",
    "\n",
    "####Q.19 Write a solution to find each query_name, the quality and poor_query_percentage.\n",
    "\n",
    "Both quality and poor_query_percentage should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Queries table:\n",
    "\n",
    "| query_name | result            | position | rating |\n",
    "|------------|-------------------|----------|--------|\n",
    "| Dog        | Golden Retriever  | 1        | 5      |\n",
    "| Dog        | German Shepherd   | 2        | 5      |\n",
    "| Dog        | Mule              | 200      | 1      |\n",
    "| Cat        | Shirazi           | 5        | 2      |\n",
    "| Cat        | Siamese           | 3        | 3      |\n",
    "| Cat        | Sphynx            | 7        | 4      |\n",
    "\n",
    "####Output: \n",
    "| query_name | quality | poor_query_percentage |\n",
    "|------------|---------|-----------------------|\n",
    "| Dog        | 2.50    | 33.33                 |\n",
    "| Cat        | 0.66    | 33.33                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad72655-ae1f-4927-bae9-be51f393e815",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>query_name</th><th>quality</th><th>poor_query_percentage</th></tr></thead><tbody><tr><td>Dog</td><td>200.6</td><td>0.33</td></tr><tr><td>Cat</td><td>5.25</td><td>0.33</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Dog",
         200.6,
         0.33
        ],
        [
         "Cat",
         5.25,
         0.33
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "query_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quality",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "poor_query_percentage",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, round, when\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (\"Dog\", \"Golden Retriever\", 1, 5),\n",
    "    (\"Dog\", \"German Shepherd\", 2, 5),\n",
    "    (\"Dog\", \"Mule\", 200, 1),\n",
    "    (\"Cat\", \"Shirazi\", 5, 2),\n",
    "    (\"Cat\", \"Siamese\", 3, 3),\n",
    "    (\"Cat\", \"Sphynx\", 7, 4)\n",
    "]\n",
    "schema = \"query_name string, result string, position int, rating int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.groupBy(col(\"query_name\")) \\\n",
    "    .agg(\n",
    "        round(sum(col(\"position\") / col(\"rating\")), 2).alias(\"quality\"),\n",
    "        round(sum(when(col(\"rating\") < 3, 1).otherwise(0)) / count(\"*\"), 2).alias(\"poor_query_percentage\")\n",
    "    ) \\\n",
    "    .select(col(\"query_name\"), col(\"quality\"), col(\"poor_query_percentage\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c6e9f3-76e1-463c-b313-3b2d7f551ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Monthly Transactions I\n",
    "####Q.20 Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Transactions table:\n",
    "\n",
    "| id   | country | state    | amount | trans_date |\n",
    "|------|---------|----------|--------|------------|\n",
    "| 121  | US      | approved | 1000   | 2018-12-18 |\n",
    "| 122  | US      | declined | 2000   | 2018-12-19 |\n",
    "| 123  | US      | approved | 2000   | 2019-01-01 |\n",
    "| 124  | DE      | approved | 2000   | 2019-01-07 |\n",
    "\n",
    "####Output: \n",
    "| month    | country | trans_count | approved_count | trans_total_amount | approved_total_amount |\n",
    "|----------|---------|-------------|----------------|--------------------|-----------------------|\n",
    "| 2018-12  | US      | 2           | 1              | 3000               | 1000                  |\n",
    "| 2019-01  | US      | 1           | 1              | 2000               | 2000                  |\n",
    "| 2019-01  | DE      | 1           | 1              | 2000               | 2000                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea63522-3f2a-4e8e-aaae-27be2b380087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>month</th><th>country</th><th>trans_count</th><th>approved_count</th><th>trans_total_amount</th><th>approved_total_amount</th></tr></thead><tbody><tr><td>2018-12</td><td>US</td><td>2</td><td>1</td><td>3000</td><td>1000</td></tr><tr><td>2019-01</td><td>US</td><td>1</td><td>1</td><td>2000</td><td>2000</td></tr><tr><td>2019-01</td><td>DE</td><td>1</td><td>1</td><td>2000</td><td>2000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2018-12",
         "US",
         2,
         1,
         3000,
         1000
        ],
        [
         "2019-01",
         "US",
         1,
         1,
         2000,
         2000
        ],
        [
         "2019-01",
         "DE",
         1,
         1,
         2000,
         2000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "trans_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "approved_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "trans_total_amount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "approved_total_amount",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format, count, sum, when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (121, \"US\", \"approved\", 1000, \"2018-12-18\"),\n",
    "    (122, \"US\", \"declined\", 2000, \"2018-12-19\"),\n",
    "    (123, \"US\", \"approved\", 2000, \"2019-01-01\"),\n",
    "    (124, \"DE\", \"approved\", 2000, \"2019-01-07\")\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"trans_date\", StringType(), True)  \n",
    "])\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df = df.withColumn(\"trans_date\", to_date(col(\"trans_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# solution\n",
    "result_df = df.groupBy(date_format(col(\"trans_date\"), \"yyyy-MM\").alias(\"month\"), col(\"country\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"trans_count\"), \n",
    "        sum(col(\"amount\")).alias(\"trans_total_amount\"), \n",
    "        sum(when(col(\"state\") == \"approved\", 1).otherwise(0)).alias(\"approved_count\"), \n",
    "        sum(when(col(\"state\") == \"approved\", col(\"amount\")).otherwise(0)).alias(\"approved_total_amount\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"month\"), \n",
    "        col(\"country\"),\n",
    "        col(\"trans_count\"), \n",
    "        col(\"approved_count\"), \n",
    "        col(\"trans_total_amount\"),  \n",
    "        col(\"approved_total_amount\")\n",
    "    )\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50bfc7c-7cde-4489-b76a-24adccc2479e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Immediate Food Delivery II\n",
    "If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "\n",
    "The first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.\n",
    "\n",
    "####Q.21 Write a solution to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Delivery table:\n",
    "\n",
    "| delivery_id | customer_id | order_date | customer_pref_delivery_date |\n",
    "|-------------|-------------|------------|-----------------------------|\n",
    "| 1           | 1           | 2019-08-01 | 2019-08-02                  |\n",
    "| 2           | 2           | 2019-08-02 | 2019-08-02                  |\n",
    "| 3           | 1           | 2019-08-11 | 2019-08-12                  |\n",
    "| 4           | 3           | 2019-08-24 | 2019-08-24                  |\n",
    "| 5           | 3           | 2019-08-21 | 2019-08-22                  |\n",
    "| 6           | 2           | 2019-08-11 | 2019-08-13                  |\n",
    "| 7           | 4           | 2019-08-09 | 2019-08-09                  |\n",
    "\n",
    "####Output: \n",
    "| immediate_percentage |\n",
    "|----------------------|\n",
    "| 50.00                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da372ac-146a-4d49-b03a-66ff5d097504",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>immediate_percentage</th></tr></thead><tbody><tr><td>50.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "50.00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "immediate_percentage",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, row_number, format_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# Input dataframe\n",
    "data = [\n",
    "    (1, 1, \"2019-08-01\", \"2019-08-02\"),\n",
    "    (2, 2, \"2019-08-02\", \"2019-08-02\"),\n",
    "    (3, 1, \"2019-08-11\", \"2019-08-12\"),\n",
    "    (4, 3, \"2019-08-24\", \"2019-08-24\"),\n",
    "    (5, 3, \"2019-08-21\", \"2019-08-22\"),\n",
    "    (6, 2, \"2019-08-11\", \"2019-08-13\"),\n",
    "    (7, 4, \"2019-08-09\", \"2019-08-09\")\n",
    "]\n",
    "schema = \"delivery_id int, customer_id int, order_date string, customer_pref_delivery_date string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df = df.withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Solution\n",
    "window_spec = Window.partitionBy(col(\"customer_id\")).orderBy(col(\"order_date\"))\n",
    "first_order_df = df.withColumn(\"order_rank\", row_number().over(window_spec)) \\\n",
    "                   .filter(col(\"order_rank\") == 1) \\\n",
    "                   .select(col(\"customer_id\"), col(\"order_date\"), col(\"customer_pref_delivery_date\"))\n",
    "first_order_count = first_order_df.count()\n",
    "immediate_orders_count = first_order_df.filter(col(\"order_date\") == col(\"customer_pref_delivery_date\")).count()\n",
    "data1 = [(first_order_count, immediate_orders_count)]\n",
    "schema1 = \"first_order_count int, immediate_orders_count int\"\n",
    "result_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "result_df = result_df.withColumn(\"immediate_percentage\",  \\\n",
    "                                 format_number((col(\"immediate_orders_count\")/col(\"first_order_count\"))*100, 2)) \\\n",
    "                                 .select(col(\"immediate_percentage\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "098dc51a-c6a0-4863-a8ee-a4fab82834a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Game Play Analysis IV\n",
    "####Q.22 Write a solution to report the fraction of players that logged in again on the day after the day they first logged in, rounded to 2 decimal places. In other words, you need to count the number of players that logged in for at least two consecutive days starting from their first login date, then divide that number by the total number of players.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Activity table:\n",
    "\n",
    "| player_id | device_id | event_date | games_played |\n",
    "|-----------|-----------|------------|--------------|\n",
    "| 1         | 2         | 2016-03-01 | 5            |\n",
    "| 1         | 2         | 2016-03-02 | 6            |\n",
    "| 2         | 3         | 2017-06-25 | 1            |\n",
    "| 3         | 1         | 2016-03-02 | 0            |\n",
    "| 3         | 4         | 2018-07-03 | 5            |\n",
    "\n",
    "####Output: \n",
    "| fraction  |\n",
    "------------|\n",
    "| 0.33      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a68ab8-43f5-4e59-b031-23fec9d14ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fraction</th></tr></thead><tbody><tr><td>0.33</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0.33
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "fraction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, lead, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 2, \"2016-03-01\", 5),\n",
    "    (1, 2, \"2016-03-02\", 6),\n",
    "    (2, 3, \"2017-06-25\", 1),\n",
    "    (3, 1, \"2016-03-02\", 0),\n",
    "    (3, 4, \"2018-07-03\", 5)\n",
    "]\n",
    "schema = \"player_id int, device_id int, event_date string, games_played int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df = df.withColumn(\"event_date\", to_date(col(\"event_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# solution\n",
    "window_spec = Window.partitionBy(col(\"player_id\")).orderBy(col(\"event_date\"))\n",
    "consecutive_login_player_count = df.withColumn(\"following_event_date\", lead(col(\"event_date\"), 1).over(window_spec)) \\\n",
    "                       .filter(col(\"event_date\")+1 == col(\"following_event_date\")).distinct().count()\n",
    "total_player_count = df.select(col(\"player_id\")).distinct().count()\n",
    "result_data = (consecutive_login_player_count/total_player_count)\n",
    "result_df = spark.createDataFrame([(result_data,)], [\"fraction\"])\n",
    "result_df = result_df.withColumn(\"fraction\", round(col(\"fraction\"), 2)).select(col(\"fraction\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa801ca-9096-4e2a-a4e0-6d744fe32622",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Number of Unique Subjects Taught by Each Teacher\n",
    "####Q.23 Write a solution to calculate the number of unique subjects each teacher teaches in the university.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is shown in the following example.\n",
    "\n",
    "####Input: \n",
    "Teacher table:\n",
    "\n",
    "| teacher_id | subject_id | dept_id |\n",
    "|------------|------------|---------|\n",
    "| 1          | 2          | 3       |\n",
    "| 1          | 2          | 4       |\n",
    "| 1          | 3          | 3       |\n",
    "| 2          | 1          | 1       |\n",
    "| 2          | 2          | 1       |\n",
    "| 2          | 3          | 1       |\n",
    "| 2          | 4          | 1       |\n",
    "\n",
    "####Output:  \n",
    "| teacher_id | cnt |\n",
    "|------------|-----|\n",
    "| 1          | 2   |\n",
    "| 2          | 4   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd7a3d19-0342-43ce-bfc8-9460d4a3139c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>teacher_id</th><th>cnt</th></tr></thead><tbody><tr><td>1</td><td>2</td></tr><tr><td>2</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2
        ],
        [
         2,
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "teacher_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "cnt",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 2, 3),\n",
    "    (1, 2, 4),\n",
    "    (1, 3, 3),\n",
    "    (2, 1, 1),\n",
    "    (2, 2, 1),\n",
    "    (2, 3, 1),\n",
    "    (2, 4, 1)\n",
    "]\n",
    "schema = \"teacher_id int, subject_id int, dept_id int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.groupBy(col(\"teacher_id\")) \\\n",
    "              .agg(countDistinct(col(\"subject_id\")).alias(\"cnt\")) \\\n",
    "              .select(col(\"teacher_id\"), col(\"cnt\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e11c399-12c4-47ea-9eed-84c337479c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###User Activity for the Past 30 Days I\n",
    "####Q.24 Write a solution to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively. A user was active on someday if they made at least one activity on that day.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Activity table:\n",
    "\n",
    "| user_id | session_id | activity_date | activity_type |\n",
    "|---------|------------|---------------|---------------|\n",
    "| 1       | 1          | 2019-07-20    | open_session  |\n",
    "| 1       | 1          | 2019-07-20    | scroll_down   |\n",
    "| 1       | 1          | 2019-07-20    | end_session   |\n",
    "| 2       | 4          | 2019-07-20    | open_session  |\n",
    "| 2       | 4          | 2019-07-21    | send_message  |\n",
    "| 2       | 4          | 2019-07-21    | end_session   |\n",
    "| 3       | 2          | 2019-07-21    | open_session  |\n",
    "| 3       | 2          | 2019-07-21    | send_message  |\n",
    "| 3       | 2          | 2019-07-21    | end_session   |\n",
    "| 4       | 3          | 2019-06-25    | open_session  |\n",
    "| 4       | 3          | 2019-06-25    | end_session   |\n",
    "\n",
    "####Output:\n",
    "| day        | active_users |\n",
    "|------------|--------------|\n",
    "| 2019-07-20 | 2            |\n",
    "| 2019-07-21 | 2            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b2d4e8-fac9-4bc3-a008-8649f45fb1c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>day</th><th>active_users</th></tr></thead><tbody><tr><td>2019-07-20</td><td>2</td></tr><tr><td>2019-07-21</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2019-07-20",
         2
        ],
        [
         "2019-07-21",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "day",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "active_users",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, countDistinct, date_sub, lit\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 1, \"2019-07-20\", \"open_session\"),\n",
    "    (1, 1, \"2019-07-20\", \"scroll_down\"),\n",
    "    (1, 1, \"2019-07-20\", \"end_session\"),\n",
    "    (2, 4, \"2019-07-20\", \"open_session\"),\n",
    "    (2, 4, \"2019-07-21\", \"send_message\"),\n",
    "    (2, 4, \"2019-07-21\", \"end_session\"),\n",
    "    (3, 2, \"2019-07-21\", \"open_session\"),\n",
    "    (3, 2, \"2019-07-21\", \"send_message\"),\n",
    "    (3, 2, \"2019-07-21\", \"end_session\"),\n",
    "    (4, 3, \"2019-06-25\", \"open_session\"),\n",
    "    (4, 3, \"2019-06-25\", \"end_session\")\n",
    "]\n",
    "schema = \"user_id int, session_id int, activity_date string, activity_type string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df = df.withColumn(\"activity_date\", to_date(col(\"activity_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# solution\n",
    "start_date = '2019-07-27'\n",
    "end_date = '2019-07-27'\n",
    "result_df = df.filter((col(\"activity_date\") >= date_sub(lit(start_date), 29)) & \\\n",
    "            (col(\"activity_date\") <= lit(end_date)) & (col(\"activity_type\").isNotNull())) \\\n",
    "            .groupBy(col(\"activity_date\").alias(\"day\")) \\\n",
    "            .agg(countDistinct(col(\"user_id\")).alias(\"active_users\")) \\\n",
    "            .select(col(\"day\"), col(\"active_users\")) \\\n",
    "            .orderBy(col(\"day\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48705432-7d73-40e3-b910-7ab609e325db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Product Sales Analysis III\n",
    "####Q.25 Write a solution to select the product id, year, quantity, and price for the first year of every product sold.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Sales table:\n",
    "\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "|---------|------------|------|----------|-------| \n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "\n",
    "Product table:\n",
    "\n",
    "| product_id | product_name |\n",
    "|------------|--------------|\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "\n",
    "####Output: \n",
    "| product_id | first_year | quantity | price |\n",
    "|------------|------------|----------|-------|\n",
    "| 100        | 2008       | 10       | 5000  |\n",
    "| 200        | 2011       | 15       | 9000  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a7d84f5-90f1-4e70-b84e-04e544c7a21c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>first_year</th><th>quantity</th><th>price</th></tr></thead><tbody><tr><td>100</td><td>2008</td><td>10</td><td>5000</td></tr><tr><td>200</td><td>2011</td><td>15</td><td>9000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         2008,
         10,
         5000
        ],
        [
         200,
         2011,
         15,
         9000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "first_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "schema = \"sale_id int, product_id int, year int, quantity int, price int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "window_spec = Window.partitionBy(col(\"product_id\")).orderBy(col(\"year\"))\n",
    "result_df = df.withColumn(\"year_rank\", row_number().over(window_spec)) \\\n",
    "            .filter(col(\"year_rank\") == 1) \\\n",
    "            .select(col(\"product_id\"), col(\"year\").alias(\"first_year\"), col(\"quantity\"), col(\"price\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade20673-010a-4a6e-842c-c972b8fa0848",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Classes More Than 5 Students\n",
    "####Q.26 Write a solution to find all the classes that have at least five students.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Courses table:\n",
    "\n",
    "| student | class    |\n",
    "|---------|----------|\n",
    "| A       | Math     |\n",
    "| B       | English  |\n",
    "| C       | Math     |\n",
    "| D       | Biology  |\n",
    "| E       | Math     |\n",
    "| F       | Computer |\n",
    "| G       | Math     |\n",
    "| H       | Math     |\n",
    "| I       | Math     |\n",
    "\n",
    "####Output:\n",
    "| class   |\n",
    "|---------|\n",
    "| Math    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d9cdd1-1b43-4774-a98e-7db052f87564",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>class</th></tr></thead><tbody><tr><td>Math</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Math"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "class",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (\"A\", \"Math\"),\n",
    "    (\"B\", \"English\"),\n",
    "    (\"C\", \"Math\"),\n",
    "    (\"D\", \"Biology\"),\n",
    "    (\"E\", \"Math\"),\n",
    "    (\"F\", \"Computer\"),\n",
    "    (\"G\", \"Math\"),\n",
    "    (\"H\", \"Math\"),\n",
    "    (\"I\", \"Math\")\n",
    "]\n",
    "schema = \"student string, class string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.groupBy(col(\"class\")) \\\n",
    "            .agg(countDistinct(col(\"student\")).alias(\"student_per_class\")) \\\n",
    "            .filter(col(\"student_per_class\") >= 5) \\\n",
    "            .select(col(\"class\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382b7309-35c7-4f6f-a908-e2e49134e09e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Find Followers Count\n",
    "####Q.27 Write a solution that will, for each user, return the number of followers.\n",
    "\n",
    "Return the result table ordered by user_id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Followers table:\n",
    "\n",
    "| user_id | follower_id |\n",
    "|---------|-------------|\n",
    "| 0       | 1           |\n",
    "| 1       | 0           |\n",
    "| 2       | 0           |\n",
    "| 2       | 1           |\n",
    "\n",
    "####Output: \n",
    "| user_id | followers_count|\n",
    "|---------|----------------|\n",
    "| 0       | 1              |\n",
    "| 1       | 1              |\n",
    "| 2       | 2              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db960bde-7697-48cc-950b-e644c2435cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>followers_count</th></tr></thead><tbody><tr><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         1
        ],
        [
         1,
         1
        ],
        [
         2,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "followers_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (0, 1),\n",
    "    (1, 0),\n",
    "    (2, 0),\n",
    "    (2, 1)\n",
    "]\n",
    "schema = \"user_id int, follower_id int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.groupBy(col(\"user_id\")) \\\n",
    "            .agg(countDistinct(col(\"follower_id\")).alias(\"followers_count\")) \\\n",
    "            .select(col(\"user_id\"), col(\"followers_count\")) \\\n",
    "            .orderBy(col(\"user_id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0ecc13-a2cc-44fd-ae75-e212ff0ea19e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Biggest Single Number\n",
    "####Q.28 A single number is a number that appeared only once in the MyNumbers table. Find the largest single number. If there is no single number, report null.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "MyNumbers table:\n",
    "\n",
    "| num |\n",
    "|-----|\n",
    "| 8   |\n",
    "| 8   |\n",
    "| 3   |\n",
    "| 3   |\n",
    "| 1   |\n",
    "| 4   |\n",
    "| 5   |\n",
    "| 6   |\n",
    "\n",
    "####Output:\n",
    "| num |\n",
    "|-----|\n",
    "| 6   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a8f8eb-8ec7-4a81-98b7-ca691161546f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num</th></tr></thead><tbody><tr><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, max\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (8,),\n",
    "    (8,),\n",
    "    (3,),\n",
    "    (3,),\n",
    "    (1,),\n",
    "    (4,),\n",
    "    (5,),\n",
    "    (6,)\n",
    "]\n",
    "schema = \"num int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "filter_df = df.groupBy(col(\"num\")) \\\n",
    "            .agg(count(col(\"*\")).alias(\"num_occurance\")) \\\n",
    "            .filter(col(\"num_occurance\") == 1) \n",
    "result_df = filter_df.select(col(\"num\")).agg(max(col(\"num\")).alias(\"num\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb306c6e-facc-472f-84ff-5bc6f1ea4a9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Customers Who Bought All Products\n",
    "####Q.29 Write a solution to report the customer ids from the Customer table that bought all the products in the Product table.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Customer table:\n",
    "\n",
    "| customer_id | product_key |\n",
    "|-------------|-------------|\n",
    "| 1           | 5           |\n",
    "| 2           | 6           |\n",
    "| 3           | 5           |\n",
    "| 3           | 6           |\n",
    "| 1           | 6           |\n",
    "\n",
    "Product table:\n",
    "\n",
    "| product_key |\n",
    "|-------------|\n",
    "| 5           |\n",
    "| 6           |\n",
    "\n",
    "####Output:\n",
    "| customer_id |\n",
    "|-------------|\n",
    "| 1           |\n",
    "| 3           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d93acb0-2a54-43c7-b8da-a54c52983a64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1
        ],
        [
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [\n",
    "    (1, 5),\n",
    "    (2, 6),\n",
    "    (3, 5),\n",
    "    (3, 6),\n",
    "    (1, 6)\n",
    "]\n",
    "schema1 = \"customer_id int, product_key int\"\n",
    "customer_df = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (5,),\n",
    "    (6,)\n",
    "]\n",
    "schema2 = \"product_key int\"\n",
    "product_df = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "unique_product_count = product_df.select(col(\"product_key\")).count()\n",
    "result_df = customer_df.groupBy(col(\"customer_id\")) \\\n",
    "            .agg(countDistinct(col(\"product_key\")).alias(\"purchase_count\")) \\\n",
    "            .filter(col(\"purchase_count\") == unique_product_count) \\\n",
    "            .select(col(\"customer_id\")) \\\n",
    "            .orderBy(col(\"customer_id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8a429b-ce14-4c80-b9dc-423f3c7e21f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The Number of Employees Which Report to Each Employee\n",
    "####Q.30 For this problem, we will consider a manager an employee who has at least 1 other employee reporting to them. Write a solution to report the ids and the names of all managers, the number of employees who report directly to them, and the average age of the reports rounded to the nearest integer.\n",
    "\n",
    "Return the result table ordered by employee_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Employees table:\n",
    "\n",
    "| employee_id | name    | reports_to | age |\n",
    "|-------------|---------|------------|-----|\n",
    "| 9           | Hercy   | null       | 43  |\n",
    "| 6           | Alice   | 9          | 41  |\n",
    "| 4           | Bob     | 9          | 36  |\n",
    "| 2           | Winston | null       | 37  |\n",
    "\n",
    "####Output: \n",
    "| employee_id | name  | reports_count | average_age |\n",
    "|-------------|-------|---------------|-------------|\n",
    "| 9           | Hercy | 2             | 39          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e05f2f-1082-44e9-b388-1c339573c242",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>reports_count</th><th>average_age</th></tr></thead><tbody><tr><td>9</td><td>Hercy</td><td>2</td><td>39.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         9,
         "Hercy",
         2,
         39.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "reports_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "average_age",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, round\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# Input DataFrame\n",
    "data = [\n",
    "    (9, \"Hercy\", None, 43),\n",
    "    (6, \"Alice\", 9, 41),\n",
    "    (4, \"Bob\", 9, 36),\n",
    "    (2, \"Winston\", None, 37)\n",
    "]\n",
    "schema = \"employee_id int, name string, reports_to int, age int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df = df.alias(\"df\")\n",
    "\n",
    "# solution\n",
    "manager_reports_df = df.groupBy(\"reports_to\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reports_count\"),\n",
    "        round(avg(\"age\"), 0).alias(\"average_age\")\n",
    "    ) \\\n",
    "    .filter(col(\"reports_to\").isNotNull()) \\\n",
    "    .alias(\"mgr\")\n",
    "result_df = manager_reports_df.join(df, col(\"mgr.reports_to\") == col(\"df.employee_id\")) \\\n",
    "    .select(\n",
    "        col(\"df.employee_id\"),\n",
    "        col(\"df.name\"),\n",
    "        col(\"mgr.reports_count\"),\n",
    "        col(\"mgr.average_age\")\n",
    "    ) \\\n",
    "    .orderBy(\"df.employee_id\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a12f54-fb52-4e05-9c52-6786cc36a1e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Primary Department for Each Employee\n",
    "####Q.31 Employees can belong to multiple departments. When the employee joins other departments, they need to decide which department is their primary department. Note that when an employee belongs to only one department, their primary column is 'N'.Write a solution to report all the employees with their primary department. For employees who belong to one department, report their only department.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Employee table:\n",
    "\n",
    "| employee_id | department_id | primary_flag |\n",
    "|-------------|---------------|--------------|\n",
    "| 1           | 1             | N            |\n",
    "| 2           | 1             | Y            |\n",
    "| 2           | 2             | N            |\n",
    "| 3           | 3             | N            |\n",
    "| 4           | 2             | N            |\n",
    "| 4           | 3             | Y            |\n",
    "| 4           | 4             | N            |\n",
    "\n",
    "####Output: \n",
    "| employee_id | department_id |\n",
    "|-------------|---------------|\n",
    "| 1           | 1             |\n",
    "| 2           | 1             |\n",
    "| 3           | 3             |\n",
    "| 4           | 3             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6be4bcb-9836-493b-ab70-28a6797b449c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>department_id</th></tr></thead><tbody><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>1</td></tr><tr><td>3</td><td>3</td></tr><tr><td>4</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1
        ],
        [
         2,
         1
        ],
        [
         3,
         3
        ],
        [
         4,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"PrimaryDepartment\").getOrCreate()\n",
    "\n",
    "# Input dataframe\n",
    "data = [\n",
    "    (1, 1, \"N\"),\n",
    "    (2, 1, \"Y\"),\n",
    "    (2, 2, \"N\"),\n",
    "    (3, 3, \"N\"),\n",
    "    (4, 2, \"N\"),\n",
    "    (4, 3, \"Y\"),\n",
    "    (4, 4, \"N\")\n",
    "]\n",
    "schema = \"employee_id int, department_id int, primary_flag string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "dept_count_df = df.groupBy(\"employee_id\").agg(count(\"*\").alias(\"dept_count\"))\n",
    "joined_df = df.join(dept_count_df, \"employee_id\")\n",
    "result_df = joined_df.filter(\n",
    "    (col(\"primary_flag\") == \"Y\") |\n",
    "    (col(\"dept_count\") == 1)\n",
    ").select(col(\"employee_id\"), col(\"department_id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8de8c07-79f1-4e7f-9aac-eb41d52d7ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Triangle Judgement\n",
    "####Q.32 Report for every three line segments whether they can form a triangle.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Triangle table:\n",
    "\n",
    "| x  | y  | z  |\n",
    "|----|----|----|\n",
    "| 13 | 15 | 30 |\n",
    "| 10 | 20 | 15 |\n",
    "\n",
    "####Output: \n",
    "| x  | y  | z  | triangle |\n",
    "|----|----|----|----------|\n",
    "| 13 | 15 | 30 | No       |\n",
    "| 10 | 20 | 15 | Yes      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f47a8ce-99cd-43b3-af80-427946c238e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>x</th><th>y</th><th>z</th><th>triangle</th></tr></thead><tbody><tr><td>13</td><td>15</td><td>30</td><td>No</td></tr><tr><td>10</td><td>20</td><td>15</td><td>Yes</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         13,
         15,
         30,
         "No"
        ],
        [
         10,
         20,
         15,
         "Yes"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "x",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "y",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "z",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "triangle",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (13, 15, 30),\n",
    "    (10, 20, 15)\n",
    "]\n",
    "schema = \"x int, y int, z int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.withColumn(\n",
    "        \"triangle\", \n",
    "        when((((col(\"x\")+col(\"y\")) > col(\"z\")) & ((col(\"y\")+col(\"z\")) > col(\"x\")) & ((col(\"z\")+col(\"x\")) > col(\"y\"))), \"Yes\")\n",
    "        .otherwise(\"No\")\n",
    "    ) \\\n",
    "    .select(col(\"x\"), col(\"y\"), col(\"z\"), \"triangle\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e60918-5acc-4795-af16-71d08a57a5d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Consecutive Numbers\n",
    "####Q.33 Find all numbers that appear at least three times consecutively.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Logs table:\n",
    "\n",
    "| id | num |\n",
    "|----|-----|\n",
    "| 1  | 1   |\n",
    "| 2  | 1   |\n",
    "| 3  | 1   |\n",
    "| 4  | 2   |\n",
    "| 5  | 1   |\n",
    "| 6  | 2   |\n",
    "| 7  | 2   |\n",
    "\n",
    "####Output: \n",
    "| ConsecutiveNums |\n",
    "|-----------------|\n",
    "| 1               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9998a616-f113-4012-93d3-ec399dafc4be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ConsecutiveNums</th></tr></thead><tbody><tr><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ConsecutiveNums",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 1),\n",
    "    (2, 1),\n",
    "    (3, 1),\n",
    "    (4, 2),\n",
    "    (5, 1),\n",
    "    (6, 2),\n",
    "    (7, 2)\n",
    "]\n",
    "schema = \"id int, num int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "window_spec = Window.orderBy(\"id\")\n",
    "df_with_lags = df.withColumn(\"prev_num1\", lag(\"num\", 1).over(window_spec)) \\\n",
    "                 .withColumn(\"prev_num2\", lag(\"num\", 2).over(window_spec))\n",
    "consecutive_nums = df_with_lags.filter(\n",
    "    (col(\"num\") == col(\"prev_num1\")) & (col(\"num\") == col(\"prev_num2\"))\n",
    ").select(col(\"num\").alias(\"ConsecutiveNums\")).distinct()\n",
    "display(consecutive_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5cb7f8-725a-4f3b-a498-6912fc0d6f11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Product Price at a Given Date\n",
    "####Q.34 Write a solution to find the prices of all products on 2019-08-16. Assume the price of all products before any change is 10.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Products table:\n",
    "\n",
    "| product_id | new_price | change_date |\n",
    "|------------|-----------|-------------|\n",
    "| 1          | 20        | 2019-08-14  |\n",
    "| 2          | 50        | 2019-08-14  |\n",
    "| 1          | 30        | 2019-08-15  |\n",
    "| 1          | 35        | 2019-08-16  |\n",
    "| 2          | 65        | 2019-08-17  |\n",
    "| 3          | 20        | 2019-08-18  |\n",
    "\n",
    "####Output: \n",
    "| product_id | price |\n",
    "|------------|-------|\n",
    "| 2          | 50    |\n",
    "| 1          | 35    |\n",
    "| 3          | 10    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fc842a-e083-4b86-ba48-15cc40c96abb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>price</th></tr></thead><tbody><tr><td>2</td><td>50</td></tr><tr><td>1</td><td>35</td></tr><tr><td>3</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         50
        ],
        [
         1,
         35
        ],
        [
         3,
         10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, max, lit, coalesce, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"ProductPriceApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 20, '2019-08-14'),\n",
    "    (2, 50, '2019-08-14'),\n",
    "    (1, 30, '2019-08-15'),\n",
    "    (1, 35, '2019-08-16'),\n",
    "    (2, 65, '2019-08-17'),\n",
    "    (3, 20, '2019-08-18')\n",
    "]\n",
    "schema = \"product_id int, new_price int, change_date string\"\n",
    "products_df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "products_df = products_df.withColumn(\"change_date\", to_date(col(\"change_date\"), \"yyyy-MM-dd\"))\n",
    "filtered_df = products_df.filter(col(\"change_date\") <= \"2019-08-16\")\n",
    "window_spec = Window.partitionBy(col(\"product_id\")).orderBy(col(\"change_date\").desc())\n",
    "latest_price_df = filtered_df.withColumn(\"latest_price\", max(col(\"new_price\")).over(window_spec)) \\\n",
    "    .filter(col(\"latest_price\").isNotNull()) \\\n",
    "    .groupBy(col(\"product_id\")).agg(max(col(\"latest_price\")).alias(\"price\"))\n",
    "all_products_df = products_df.select(col(\"product_id\")).distinct()\n",
    "result_df = all_products_df.join(latest_price_df, on=\"product_id\", how=\"left_outer\") \\\n",
    "    .withColumn(\"price\", coalesce(col(\"price\"), lit(10)))\n",
    "result_df = result_df.select(col(\"product_id\"), col(\"price\")).orderBy(col(\"price\").desc())\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375d8636-30c2-40cb-81c9-f42bd6c22007",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Last Person to Fit in the Bus\n",
    "####Q.35 There is a queue of people waiting to board a bus. However, the bus has a weight limit of 1000 kilograms, so there may be some people who cannot board. Write a solution to find the person_name of the last person that can fit on the bus without exceeding the weight limit. The test cases are generated such that the first person does not exceed the weight limit.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Queue table:\n",
    "\n",
    "| person_id | person_name | weight | turn |\n",
    "|-----------|-------------|--------|------|\n",
    "| 5         | Alice       | 250    | 1    |\n",
    "| 4         | Bob         | 175    | 5    |\n",
    "| 3         | Alex        | 350    | 2    |\n",
    "| 6         | John Cena   | 400    | 3    |\n",
    "| 1         | Winston     | 500    | 6    |\n",
    "| 2         | Marie       | 200    | 4    |\n",
    "\n",
    "####Output: \n",
    "| person_name |\n",
    "|-------------|\n",
    "| John Cena   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4d92ab-7e08-42f8-bc00-31c5e6f13124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>person_name</th></tr></thead><tbody><tr><td>John Cena</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John Cena"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "person_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (5, \"Alice\", 250, 1),\n",
    "    (4, \"Bob\", 175, 5),\n",
    "    (3, \"Alex\", 350, 2),\n",
    "    (6, \"John Cena\", 400, 3),\n",
    "    (1, \"Winston\", 500, 6),\n",
    "    (2, \"Marie\", 200, 4)\n",
    "]\n",
    "schema = \"person_id int, person_name string, weight int, turn int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "window_spec = Window.orderBy(col(\"turn\"))\n",
    "result_df = df.withColumn(\"weight_per_turn\", sum(col(\"weight\")).over(window_spec)) \\\n",
    "    .filter(col(\"weight_per_turn\") <= 1000) \\\n",
    "    .orderBy(desc(col(\"turn\"))) \\\n",
    "    .select(col(\"person_name\")).limit(1)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb99beb-7790-4f08-a185-f7ec90ca01ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Count Salary Categories\n",
    "####Q.36 Write a solution to calculate the number of bank accounts for each salary category. The salary categories are:\n",
    "- **\"Low Salary\":** All the salaries strictly less than $20000.\n",
    "- **\"Average Salary\":** All the salaries in the inclusive range [$20000, $50000].\n",
    "- **\"High Salary\":** All the salaries strictly greater than $50000.\n",
    "\n",
    "The result table must contain all three categories. If there are no accounts in a category, return 0.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Accounts table:\n",
    "\n",
    "| account_id | income |\n",
    "|------------|--------|\n",
    "| 3          | 108939 |\n",
    "| 2          | 12747  |\n",
    "| 8          | 87709  |\n",
    "| 6          | 91796  |\n",
    "\n",
    "####Output: \n",
    "| category       | accounts_count |\n",
    "|----------------|----------------|\n",
    "| Low Salary     | 1              |\n",
    "| Average Salary | 0              |\n",
    "| High Salary    | 3              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09fa9b35-c87e-4bc5-9128-779a88085977",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>salary_category</th><th>accounts_count</th></tr></thead><tbody><tr><td>Low Salary</td><td>1</td></tr><tr><td>Average Salary</td><td>0</td></tr><tr><td>High Salary</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Low Salary",
         1
        ],
        [
         "Average Salary",
         0
        ],
        [
         "High Salary",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "salary_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "accounts_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (3, 108939),\n",
    "    (2, 12747),\n",
    "    (8, 87709),\n",
    "    (6, 91796)\n",
    "]\n",
    "schema = \"account_id int, income int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "salary_category_df = df.withColumn(\"salary_category\", \n",
    "                                   when(col(\"income\") < 20000, \"Low Salary\") \n",
    "                                   .when((col(\"income\") >= 20000) & (col(\"income\") <= 50000), \"Average Salary\") \n",
    "                                   .otherwise(\"High Salary\"))\n",
    "category_count_df = salary_category_df.groupBy(col(\"salary_category\")) \\\n",
    "            .agg(count(col(\"*\")).alias(\"accounts_count\")) \\\n",
    "            .select(col(\"salary_category\"), col(\"accounts_count\"))\n",
    "categories = [\"Low Salary\", \"Average Salary\", \"High Salary\"]\n",
    "category_df = spark.createDataFrame([(cat,) for cat in categories], [\"salary_category\"])\n",
    "result_df = category_df.join(category_count_df, on=\"salary_category\", how=\"left_outer\").fillna(0)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654aa38b-e7d0-4927-8c23-dbeed9b3f3d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Employees Whose Manager Left the Company\n",
    "####Q.37 Find the IDs of the employees whose salary is strictly less than $30000 and whose manager left the company. When a manager leaves the company, their information is deleted from the Employees table, but the reports still have their manager_id set to the manager that left.\n",
    "\n",
    "Return the result table ordered by employee_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input:  \n",
    "Employees table:\n",
    "\n",
    "| employee_id | name      | manager_id | salary |\n",
    "|-------------|-----------|------------|--------|\n",
    "| 3           | Mila      | 9          | 60301  |\n",
    "| 12          | Antonella | null       | 31000  |\n",
    "| 13          | Emery     | null       | 67084  |\n",
    "| 1           | Kalel     | 11         | 21241  |\n",
    "| 9           | Mikaela   | null       | 50937  |\n",
    "| 11          | Joziah    | 6          | 28485  |\n",
    "\n",
    "####Output:\n",
    "| employee_id |\n",
    "|-------------|\n",
    "| 11          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab187162-ab0c-4097-95c6-80ad4c28b1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th></tr></thead><tbody><tr><td>11</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         11
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (3, \"Mila\", 9, 60301),\n",
    "    (12, \"Antonella\", None, 31000),\n",
    "    (13, \"Emery\", None, 67084),\n",
    "    (1, \"Kalel\", 11, 21241),\n",
    "    (9, \"Mikaela\", None, 50937),\n",
    "    (11, \"Joziah\", 6, 28485)\n",
    "]\n",
    "schema = \"employee_id int, name string, manager_id int, salary int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "all_employee_df = df.select(col(\"employee_id\")).distinct()\n",
    "filter_df = df.filter((col(\"salary\") < 30000) & (col(\"manager_id\").isNotNull())) \\\n",
    "            .select(col(\"employee_id\"), col(\"manager_id\"))\n",
    "manager_left_df = filter_df.select(col(\"manager_id\")).subtract(all_employee_df).alias(\"manager_id\")\n",
    "result_df = manager_left_df.join(filter_df, on=\"manager_id\", how=\"inner\").select(col(\"employee_id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b55716c-94ec-419d-a8b6-213184ba4215",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Exchange Seats\n",
    "####Q.38 Write a solution to swap the seat id of every two consecutive students. If the number of students is odd, the id of the last student is not swapped.\n",
    "\n",
    "Return the result table ordered by id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Seat table:\n",
    "\n",
    "| id | student |\n",
    "|----|---------|\n",
    "| 1  | Abbot   |\n",
    "| 2  | Doris   |\n",
    "| 3  | Emerson |\n",
    "| 4  | Green   |\n",
    "| 5  | Jeames  |\n",
    "\n",
    "####Output: \n",
    "| id | student |\n",
    "|----|---------|\n",
    "| 1  | Doris   |\n",
    "| 2  | Abbot   |\n",
    "| 3  | Green   |\n",
    "| 4  | Emerson |\n",
    "| 5  | Jeames  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5535ab-51e6-4085-a6f0-0b36f054003d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>student</th></tr></thead><tbody><tr><td>1</td><td>Doris</td></tr><tr><td>2</td><td>Abbot</td></tr><tr><td>3</td><td>Green</td></tr><tr><td>4</td><td>Emerson</td></tr><tr><td>5</td><td>Jeames</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Doris"
        ],
        [
         2,
         "Abbot"
        ],
        [
         3,
         "Green"
        ],
        [
         4,
         "Emerson"
        ],
        [
         5,
         "Jeames"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "student",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, \"Abbot\"),\n",
    "    (2, \"Doris\"),\n",
    "    (3, \"Emerson\"),\n",
    "    (4, \"Green\"),\n",
    "    (5, \"Jeames\")\n",
    "]\n",
    "schema = \"id int, student string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "student_count = df.count()\n",
    "result_df = df.withColumn(\"new_id\", \n",
    "                          when((col(\"id\") % 2 == 0), col(\"id\") - 1)\n",
    "                          .when((col(\"id\") % 2 != 0) & (col(\"id\") < student_count), col(\"id\") + 1)\n",
    "                          .otherwise(col(\"id\"))) \\\n",
    "                         .select(col(\"new_id\").alias(\"id\"), col(\"student\")).orderBy(col(\"id\"))\n",
    "display(result_df)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c9a003-5142-4586-9218-6786bab0bb50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Movie Rating\n",
    "####Q.39 Write a solution to:\n",
    "\n",
    "- Find the name of the user who has rated the greatest number of movies. In case of a tie, return the lexicographically smaller user name.\n",
    "\n",
    "- Find the movie name with the highest average rating in February 2020. In case of a tie, return the lexicographically smaller movie name.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Movies table:\n",
    "\n",
    "| movie_id    |  title       |\n",
    "|-------------|--------------|\n",
    "| 1           | Avengers     |\n",
    "| 2           | Frozen 2     |\n",
    "| 3           | Joker        |\n",
    "\n",
    "Users table:\n",
    "\n",
    "| user_id     |  name        |\n",
    "|-------------|--------------|\n",
    "| 1           | Daniel       |\n",
    "| 2           | Monica       |\n",
    "| 3           | Maria        |\n",
    "| 4           | James        |\n",
    "\n",
    "MovieRating table:\n",
    "\n",
    "| movie_id    | user_id      | rating       | created_at  |\n",
    "|-------------|--------------|--------------|-------------|\n",
    "| 1           | 1            | 3            | 2020-01-12  |\n",
    "| 1           | 2            | 4            | 2020-02-11  |\n",
    "| 1           | 3            | 2            | 2020-02-12  |\n",
    "| 1           | 4            | 1            | 2020-01-01  |\n",
    "| 2           | 1            | 5            | 2020-02-17  | \n",
    "| 2           | 2            | 2            | 2020-02-01  | \n",
    "| 2           | 3            | 2            | 2020-03-01  |\n",
    "| 3           | 1            | 3            | 2020-02-22  | \n",
    "| 3           | 2            | 4            | 2020-02-25  | \n",
    "\n",
    "####Output: \n",
    "| results      |\n",
    "|--------------|\n",
    "| Daniel       |\n",
    "| Frozen 2     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5943b4e-9803-485d-9f76-c19cec96719f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>results</th></tr></thead><tbody><tr><td>Daniel</td></tr><tr><td>Frozen 2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Daniel"
        ],
        [
         "Frozen 2"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "results",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, month, year, count, avg, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"MovieRatingApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "movies_data = [\n",
    "    (1, 'Avengers'),\n",
    "    (2, 'Frozen 2'),\n",
    "    (3, 'Joker')\n",
    "]\n",
    "movies_schema = \"movie_id int, title string\"\n",
    "movies_df = spark.createDataFrame(data=movies_data, schema=movies_schema)\n",
    "users_data = [\n",
    "    (1, 'Daniel'),\n",
    "    (2, 'Monica'),\n",
    "    (3, 'Maria'),\n",
    "    (4, 'James')\n",
    "]\n",
    "users_schema = \"user_id int, name string\"\n",
    "users_df = spark.createDataFrame(data=users_data, schema=users_schema)\n",
    "ratings_data = [\n",
    "    (1, 1, 3, '2020-01-12'),\n",
    "    (1, 2, 4, '2020-02-11'),\n",
    "    (1, 3, 2, '2020-02-12'),\n",
    "    (1, 4, 1, '2020-01-01'),\n",
    "    (2, 1, 5, '2020-02-17'),\n",
    "    (2, 2, 2, '2020-02-01'),\n",
    "    (2, 3, 2, '2020-03-01'),\n",
    "    (3, 1, 3, '2020-02-22'),\n",
    "    (3, 2, 4, '2020-02-25')\n",
    "]\n",
    "ratings_schema = \"movie_id int, user_id int, rating int, created_at string\"\n",
    "ratings_df = spark.createDataFrame(data=ratings_data, schema=ratings_schema)\n",
    "\n",
    "# solution\n",
    "ratings_df = ratings_df.withColumn(\"created_at\", to_date(col(\"created_at\"), \"yyyy-MM-dd\"))\n",
    "user_rating_count_df = ratings_df.groupBy(col(\"user_id\")).agg(count(\"*\").alias(\"rating_count\"))\n",
    "most_ratings_user_df = user_rating_count_df.join(users_df, on=\"user_id\") \\\n",
    "                                           .orderBy(col(\"rating_count\").desc(), col(\"name\").asc()) \\\n",
    "                                           .limit(1)\n",
    "february_ratings_df = ratings_df.filter((month(col(\"created_at\")) == 2) & (year(col(\"created_at\")) == 2020))\n",
    "movie_avg_rating_df = february_ratings_df.groupBy(col(\"movie_id\")).agg(avg(col(\"rating\")).alias(\"avg_rating\"))\n",
    "highest_avg_rating_movie_df = movie_avg_rating_df.join(movies_df, on=\"movie_id\") \\\n",
    "                                                 .orderBy(col(\"avg_rating\").desc(), col(\"title\").asc()) \\\n",
    "                                                 .limit(1)\n",
    "result_df = most_ratings_user_df.select(col(\"name\").alias(\"results\")) \\\n",
    "    .union(highest_avg_rating_movie_df.select(col(\"title\").alias(\"results\")))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1095e2c5-138b-4568-9638-aa569c42f888",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Restaurant Growth\n",
    "####Q.40 You are the restaurant owner and you want to analyze a possible expansion (there will be at least one customer every day). \n",
    "\n",
    "Compute the moving average of how much the customer paid in a seven days window (i.e., current day + 6 days before). average_amount should be rounded to two decimal places.\n",
    "\n",
    "Return the result table ordered by visited_on in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Customer table:\n",
    "\n",
    "| customer_id | name         | visited_on   | amount      |\n",
    "|-------------|--------------|--------------|-------------|\n",
    "| 1           | Jhon         | 2019-01-01   | 100         |\n",
    "| 2           | Daniel       | 2019-01-02   | 110         |\n",
    "| 3           | Jade         | 2019-01-03   | 120         |\n",
    "| 4           | Khaled       | 2019-01-04   | 130         |\n",
    "| 5           | Winston      | 2019-01-05   | 110         | \n",
    "| 6           | Elvis        | 2019-01-06   | 140         | \n",
    "| 7           | Anna         | 2019-01-07   | 150         |\n",
    "| 8           | Maria        | 2019-01-08   | 80          |\n",
    "| 9           | Jaze         | 2019-01-09   | 110         | \n",
    "| 1           | Jhon         | 2019-01-10   | 130         | \n",
    "| 3           | Jade         | 2019-01-10   | 150         | \n",
    "\n",
    "####Output: \n",
    "| visited_on   | amount       | average_amount |\n",
    "|--------------|--------------|----------------|\n",
    "| 2019-01-07   | 860          | 122.86         |\n",
    "| 2019-01-08   | 840          | 120            |\n",
    "| 2019-01-09   | 840          | 120            |\n",
    "| 2019-01-10   | 1000         | 142.86         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fee5e5-d358-4fd9-a87e-caa729d92995",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>visited_on</th><th>amount</th><th>average_amount</th></tr></thead><tbody><tr><td>2019-01-07</td><td>860</td><td>122.86</td></tr><tr><td>2019-01-08</td><td>840</td><td>120.0</td></tr><tr><td>2019-01-09</td><td>840</td><td>120.0</td></tr><tr><td>2019-01-10</td><td>1000</td><td>142.86</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2019-01-07",
         860,
         122.86
        ],
        [
         "2019-01-08",
         840,
         120.0
        ],
        [
         "2019-01-09",
         840,
         120.0
        ],
        [
         "2019-01-10",
         1000,
         142.86
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "visited_on",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "average_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum, round, to_date, expr\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"RestaurantGrowth\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, \"Jhon\", \"2019-01-01\", 100),\n",
    "    (2, \"Daniel\", \"2019-01-02\", 110),\n",
    "    (3, \"Jade\", \"2019-01-03\", 120),\n",
    "    (4, \"Khaled\", \"2019-01-04\", 130),\n",
    "    (5, \"Winston\", \"2019-01-05\", 110),\n",
    "    (6, \"Elvis\", \"2019-01-06\", 140),\n",
    "    (7, \"Anna\", \"2019-01-07\", 150),\n",
    "    (8, \"Maria\", \"2019-01-08\", 80),\n",
    "    (9, \"Jaze\", \"2019-01-09\", 110),\n",
    "    (1, \"Jhon\", \"2019-01-10\", 130),\n",
    "    (3, \"Jade\", \"2019-01-10\", 150)\n",
    "]\n",
    "schema = \"customer_id int, name string, visited_on string, amount int\"\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"visited_on\", to_date(col(\"visited_on\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# solution\n",
    "daily_df = df.groupBy(\"visited_on\").agg(sum(\"amount\").alias(\"amount\"))\n",
    "window_spec = Window.orderBy(\"visited_on\").rowsBetween(-6, 0)\n",
    "result_df = daily_df.withColumn(\"rolling_amount\", sum(\"amount\").over(window_spec)) \\\n",
    "                    .withColumn(\"average_amount\", round(col(\"rolling_amount\") / 7, 2)) \\\n",
    "                    .select(\"visited_on\", \"rolling_amount\", \"average_amount\") \\\n",
    "                    .orderBy(\"visited_on\")\n",
    "min_date = df.agg(expr(\"min(visited_on)\")).collect()[0][0]\n",
    "result_df = result_df.filter(col(\"visited_on\") >= expr(f\"DATE_ADD('{min_date}', 6)\"))\n",
    "result_df = result_df.withColumnRenamed(\"rolling_amount\", \"amount\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabc4996-9206-4610-8704-fb0b93498e4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Friend Requests II: Who Has the Most Friends\n",
    "####Q.41 Write a solution to find the people who have the most friends and the most friends number.\n",
    "\n",
    "The test cases are generated so that only one person has the most friends.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "RequestAccepted table:\n",
    "\n",
    "| requester_id | accepter_id | accept_date |\n",
    "|--------------|-------------|-------------|\n",
    "| 1            | 2           | 2016/06/03  |\n",
    "| 1            | 3           | 2016/06/08  |\n",
    "| 2            | 3           | 2016/06/08  |\n",
    "| 3            | 4           | 2016/06/09  |\n",
    "\n",
    "####Output:\n",
    "| id | num |\n",
    "|----|-----|\n",
    "| 3  | 3   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4407286d-f929-4aff-9f29-7410287a4c47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>num</th></tr></thead><tbody><tr><td>3</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "num",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 2, '2016/06/03'),\n",
    "    (1, 3, '2016/06/08'),\n",
    "    (2, 3, '2016/06/08'),\n",
    "    (3,\t4, '2016/06/09')\n",
    "]\n",
    "schema = \"requester_id int, accepter_id int, accept_date string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "union_df = df.select(col(\"requester_id\").alias(\"id\")) \\\n",
    "     .unionAll(df.select(col(\"accepter_id\").alias(\"id\")))\n",
    "result_df = union_df.groupBy(col(\"id\")).agg(count(col(\"id\")).alias(\"num\")).orderBy(col(\"num\").desc()).limit(1)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8825d34e-f0ca-4659-b293-c3c34c86668a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Investments in 2016\n",
    "####Q.42 Write a solution to report the sum of all total investment values in 2016 tiv_2016, for all policyholders who:\n",
    "- have the same tiv_2015 value as one or more other policyholders, and\n",
    "- are not located in the same city as any other policyholder (i.e., the (lat, lon) attribute pairs must be unique).\n",
    "Round tiv_2016 to two decimal places.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Insurance table:\n",
    "\n",
    "| pid | tiv_2015 | tiv_2016 | lat | lon |\n",
    "|-----|----------|----------|-----|-----|\n",
    "| 1   | 10       | 5        | 10  | 10  |\n",
    "| 2   | 20       | 20       | 20  | 20  |\n",
    "| 3   | 10       | 30       | 20  | 20  |\n",
    "| 4   | 10       | 40       | 40  | 40  |\n",
    "\n",
    "####Output: \n",
    "| tiv_2016 |\n",
    "|----------|\n",
    "| 45.00    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18dbf79d-4ad5-4e34-b3f2-03446e34fd78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>tiv_2016</th></tr></thead><tbody><tr><td>45.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "45.00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "tiv_2016",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, format_number\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 10, 5, 10, 10),\n",
    "    (2, 20, 20, 20, 20),\n",
    "    (3, 10, 30, 20, 20),\n",
    "    (4, 10, 40, 40, 40)\n",
    "]\n",
    "schema = \"pid int, tiv_2015 int, tiv_2016 int, lat int, lon int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "duplicate_tiv_2015 = df.groupBy(col(\"tiv_2015\")).count().filter(col(\"count\") > 1).select(col(\"tiv_2015\"))\n",
    "unique_location = df.groupBy(col(\"lat\"), col(\"lon\")).count().filter(col(\"count\") == 1).select(col(\"lat\"), col(\"lon\"))\n",
    "filtered_df = df.join(duplicate_tiv_2015, on=\"tiv_2015\").join(unique_location, on=[\"lat\", \"lon\"])\n",
    "result_df = filtered_df.agg(format_number(sum(\"tiv_2016\"), 2).alias(\"tiv_2016\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32983a11-f555-4c5f-96be-9360ccdff3e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Department Top Three Salaries\n",
    "####Q.43 A company's executives are interested in seeing who earns the most money in each of the company's departments. A high earner in a department is an employee who has a salary in the top three unique salaries for that department.\n",
    "\n",
    "Write a solution to find the employees who are high earners in each of the departments.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Employee table:\n",
    "\n",
    "| id | name  | salary | departmentId |\n",
    "|----|-------|--------|--------------|\n",
    "| 1  | Joe   | 85000  | 1            |\n",
    "| 2  | Henry | 80000  | 2            |\n",
    "| 3  | Sam   | 60000  | 2            |\n",
    "| 4  | Max   | 90000  | 1            |\n",
    "| 5  | Janet | 69000  | 1            |\n",
    "| 6  | Randy | 85000  | 1            |\n",
    "| 7  | Will  | 70000  | 1            |\n",
    "\n",
    "Department table:\n",
    "\n",
    "| id | name  |\n",
    "|----|-------|\n",
    "| 1  | IT    |\n",
    "| 2  | Sales |\n",
    "\n",
    "####Output: \n",
    "| Department | Employee | Salary |\n",
    "|------------|----------|--------|\n",
    "| IT         | Max      | 90000  |\n",
    "| IT         | Joe      | 85000  |\n",
    "| IT         | Randy    | 85000  |\n",
    "| IT         | Will     | 70000  |\n",
    "| Sales      | Henry    | 80000  |\n",
    "| Sales      | Sam      | 60000  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838aafb7-5cd9-4a83-b2e9-c36eb206da60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Department</th><th>Employee</th><th>Salary</th></tr></thead><tbody><tr><td>IT</td><td>Max</td><td>90000</td></tr><tr><td>IT</td><td>Joe</td><td>85000</td></tr><tr><td>IT</td><td>Randy</td><td>85000</td></tr><tr><td>IT</td><td>Will</td><td>70000</td></tr><tr><td>Sales</td><td>Henry</td><td>80000</td></tr><tr><td>Sales</td><td>Sam</td><td>60000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "IT",
         "Max",
         90000
        ],
        [
         "IT",
         "Joe",
         85000
        ],
        [
         "IT",
         "Randy",
         85000
        ],
        [
         "IT",
         "Will",
         70000
        ],
        [
         "Sales",
         "Henry",
         80000
        ],
        [
         "Sales",
         "Sam",
         60000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Employee",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [\n",
    "    (1, \"Joe\", 85000, 1),\n",
    "    (2, \"Henry\", 80000, 2),\n",
    "    (3, \"Sam\", 60000, 2),\n",
    "    (4, \"Max\", 90000, 1),\n",
    "    (5, \"Janet\", 69000, 1),\n",
    "    (6, \"Randy\", 85000, 1),\n",
    "    (7, \"Will\", 70000, 1)\n",
    "]\n",
    "schema1 = \"id int, employee_name string, salary int, departmentId int\"\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (1, \"IT\"),\n",
    "    (2, \"Sales\")\n",
    "]\n",
    "schema2 = \"id int, department_name string\"\n",
    "df2 = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# solution\n",
    "joined_df = df1.join(df2, df1.departmentId == df2.id, \"left_outer\")\n",
    "window_spec = Window.partitionBy(\"department_name\").orderBy(col(\"salary\").desc())\n",
    "ranked_df = joined_df.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "result_df = ranked_df.filter(col(\"rank\") <= 3) \\\n",
    "            .select(col(\"department_name\").alias(\"Department\"), col(\"employee_name\").alias(\"Employee\"), \\\n",
    "            col(\"salary\").alias(\"Salary\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dae036d-f521-4e00-a380-a43281a76968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Fix Names in a Table\n",
    "####Q.44 Write a solution to fix the names so that only the first character is uppercase and the rest are lowercase.\n",
    "\n",
    "Return the result table ordered by user_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input:\n",
    "Users table:\n",
    "\n",
    "| user_id | name  |\n",
    "|---------|-------|\n",
    "| 1       | aLice |\n",
    "| 2       | bOB   |\n",
    "\n",
    "####Output: \n",
    "| user_id | name  |\n",
    "|---------|-------|\n",
    "| 1       | Alice |\n",
    "| 2       | Bob   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28399b62-73c4-4512-ac2c-1c274dc6531f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>Alice</td></tr><tr><td>2</td><td>Bob</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice"
        ],
        [
         2,
         "Bob"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat, substring, lower, upper\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 'aLice'),\n",
    "    (2, 'bOB')\n",
    "]\n",
    "schema = \"user_id int, name string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.withColumn(\"name\", concat(upper(substring(col(\"name\"), 1, 1)), lower(substring(col(\"name\"), 2, 100)))).orderBy(col(\"name\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f69781f-d38d-4539-873b-c79734f8f1bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Patients With a Condition\n",
    "####Q.45 Write a solution to find the patient_id, patient_name, and conditions of the patients who have Type I Diabetes. Type I Diabetes always starts with DIAB1 prefix.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Patients table:\n",
    "\n",
    "| patient_id | patient_name | conditions   |\n",
    "|------------|--------------|--------------|\n",
    "| 1          | Daniel       | YFEV COUGH   |\n",
    "| 2          | Alice        |              |\n",
    "| 3          | Bob          | DIAB100 MYOP |\n",
    "| 4          | George       | ACNE DIAB100 |\n",
    "| 5          | Alain        | DIAB201      |\n",
    "\n",
    "####Output: \n",
    "| patient_id | patient_name | conditions   |\n",
    "|------------|--------------|--------------|\n",
    "| 3          | Bob          | DIAB100 MYOP |\n",
    "| 4          | George       | ACNE DIAB100 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb71798-f1c8-468b-a05d-459667346bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>patient_id</th><th>patient_name</th><th>conditions</th></tr></thead><tbody><tr><td>3</td><td>Bob</td><td>DIAB100 MYOP</td></tr><tr><td>4</td><td>George</td><td>ACNE DIAB100</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Bob",
         "DIAB100 MYOP"
        ],
        [
         4,
         "George",
         "ACNE DIAB100"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "patient_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "conditions",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 'Daniel', 'YFEV COUGH'),\n",
    "    (2, 'Alice', None),\t\n",
    "    (3,\t'Bob', 'DIAB100 MYOP'),\n",
    "    (4, 'George', 'ACNE DIAB100'),\n",
    "    (5,\t'Alain', 'DIAB201')\n",
    "]\n",
    "schema = \"patient_id int, patient_name string, conditions string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "result_df = df.filter(col(\"conditions\").like(\"%DIAB1%\")).select(col(\"patient_id\"), col(\"patient_name\"), col(\"conditions\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ef484e-033b-41e1-b938-3187d439c77b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Delete Duplicate Emails\n",
    "####Q.46 Write a solution to delete all duplicate emails, keeping only one unique email with the smallest id. \n",
    "\n",
    "For SQL users, please note that you are supposed to write a DELETE statement and not a SELECT one.\n",
    "\n",
    "For Pandas users, please note that you are supposed to modify Person in place.\n",
    "\n",
    "After running your script, the answer shown is the Person table. The driver will first compile and run your piece of code and then show the Person table. The final order of the Person table does not matter.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Person table:\n",
    "\n",
    "| id | email            |\n",
    "|----|------------------|\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |\n",
    "| 3  | john@example.com |\n",
    "\n",
    "####Output: \n",
    "| id | email            |\n",
    "|----|------------------|\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1be92ff-b18f-407b-97e7-acd86be6d1e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>email</th></tr></thead><tbody><tr><td>1</td><td>john@example.com</td></tr><tr><td>2</td><td>bob@example.com</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "john@example.com"
        ],
        [
         2,
         "bob@example.com"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 'john@example.com'),\n",
    "    (2, 'bob@example.com'),\n",
    "    (3, 'john@example.com')\n",
    "]\n",
    "schema = \"id int, email string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "window_spec = Window.orderBy(col(\"id\")).partitionBy(col(\"email\"))\n",
    "result_df = df.withColumn(\"ranking\", row_number().over(window_spec)).filter(col(\"ranking\") == 1) \\\n",
    "        .select(col(\"id\"), col(\"email\")).orderBy(col(\"id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a8f27d-f3cd-4e1b-9587-7478443b9c13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Second Highest Salary\n",
    "####Q.47 Write a solution to find the second highest salary from the Employee table. If there is no second highest salary, return null (return None in Pandas).\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Employee table:\n",
    "\n",
    "| id | salary |\n",
    "|----|--------|\n",
    "| 1  | 100    |\n",
    "| 2  | 200    |\n",
    "| 3  | 300    |\n",
    "\n",
    "####Output: \n",
    "| SecondHighestSalary |\n",
    "|---------------------|\n",
    "| 200                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9203a7-5c68-4698-9415-3524f1a63169",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>SecondHighestSalary</th></tr></thead><tbody><tr><td>200</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         200
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "SecondHighestSalary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import col, desc, dense_rank, when, max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 300)\n",
    "]\n",
    "schema = \"id int, salary int\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "window_spec = Window.orderBy(col(\"salary\").desc())\n",
    "rank_df = df.withColumn(\"dense_ranking\", dense_rank().over(window_spec))\n",
    "max_rank = rank_df.select(max(col(\"dense_ranking\"))).collect()[0][0]\n",
    "result_df = rank_df.withColumn(\n",
    "    \"SecondHighestSalary\",\n",
    "    when(col(\"dense_ranking\") >= 2, col(\"salary\"))\n",
    ").filter(col(\"dense_ranking\") == 2).select(col(\"SecondHighestSalary\")).distinct()\n",
    "if max_rank < 2:\n",
    "    result_df = spark.createDataFrame([(None,)], StructType([StructField(\"SecondHighestSalary\", IntegerType(), True)]))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d84dcf24-6788-4948-9493-a74b1456bbee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Group Sold Products By The Date\n",
    "####Q.48 Write a solution to find for each date the number of different products sold and their names.\n",
    "\n",
    "The sold products names for each date should be sorted lexicographically.\n",
    "\n",
    "Return the result table ordered by sell_date.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Activities table:\n",
    "\n",
    "| sell_date  | product    |\n",
    "|------------|------------|\n",
    "| 2020-05-30 | Headphone  |\n",
    "| 2020-06-01 | Pencil     |\n",
    "| 2020-06-02 | Mask       |\n",
    "| 2020-05-30 | Basketball |\n",
    "| 2020-06-01 | Bible      |\n",
    "| 2020-06-02 | Mask       |\n",
    "| 2020-05-30 | T-Shirt    |\n",
    "\n",
    "####Output: \n",
    "| sell_date  | num_sold | products                     |\n",
    "|------------|----------|------------------------------|\n",
    "| 2020-05-30 | 3        | Basketball,Headphone,T-shirt |\n",
    "| 2020-06-01 | 2        | Bible,Pencil                 |\n",
    "| 2020-06-02 | 1        | Mask                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a67952-e2d4-4874-ab9a-4a602a2b7750",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sell_date</th><th>num_sold</th><th>products</th></tr></thead><tbody><tr><td>2020-05-30</td><td>3</td><td>Basketball,Headphone,T-Shirt</td></tr><tr><td>2020-06-01</td><td>2</td><td>Bible,Pencil</td></tr><tr><td>2020-06-02</td><td>1</td><td>Mask</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2020-05-30",
         3,
         "Basketball,Headphone,T-Shirt"
        ],
        [
         "2020-06-01",
         2,
         "Bible,Pencil"
        ],
        [
         "2020-06-02",
         1,
         "Mask"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sell_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "num_sold",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "products",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, countDistinct, collect_set, sort_array, array_join\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (\"2020-05-30\", \"Headphone\"),\n",
    "    (\"2020-06-01\", \"Pencil\"),\n",
    "    (\"2020-06-02\", \"Mask\"),\n",
    "    (\"2020-05-30\", \"Basketball\"),\n",
    "    (\"2020-06-01\", \"Bible\"),\n",
    "    (\"2020-06-02\", \"Mask\"),\n",
    "    (\"2020-05-30\", \"T-Shirt\")\n",
    "]\n",
    "schema = \"sell_date string, product string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df = df.withColumn(\"sell_date\", to_date(col(\"sell_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# solution\n",
    "result_df = df.groupBy(col(\"sell_date\")) \\\n",
    "    .agg(\n",
    "        countDistinct(col(\"product\")).alias(\"num_sold\"),\n",
    "        array_join(sort_array(collect_set(col(\"product\"))), delimiter=\",\").alias(\"products\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"sell_date\"))\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5863ccc-5ab2-487e-aa6a-4008e1a752c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###List the Products Ordered in a Period\n",
    "####Q.49 Write a solution to get the names of products that have at least 100 units ordered in February 2020 and their amount.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Products table:\n",
    "\n",
    "| product_id  | product_name          | product_category |\n",
    "|-------------|-----------------------|------------------|\n",
    "| 1           | Leetcode Solutions    | Book             |\n",
    "| 2           | Jewels of Stringology | Book             |\n",
    "| 3           | HP                    | Laptop           |\n",
    "| 4           | Lenovo                | Laptop           |\n",
    "| 5           | Leetcode Kit          | T-shirt          |\n",
    "\n",
    "Orders table:\n",
    "\n",
    "| product_id   | order_date   | unit     |\n",
    "|--------------|--------------|----------|\n",
    "| 1            | 2020-02-05   | 60       |\n",
    "| 1            | 2020-02-10   | 70       |\n",
    "| 2            | 2020-01-18   | 30       |\n",
    "| 2            | 2020-02-11   | 80       |\n",
    "| 3            | 2020-02-17   | 2        |\n",
    "| 3            | 2020-02-24   | 3        |\n",
    "| 4            | 2020-03-01   | 20       |\n",
    "| 4            | 2020-03-04   | 30       |\n",
    "| 4            | 2020-03-04   | 60       |\n",
    "| 5            | 2020-02-25   | 50       |\n",
    "| 5            | 2020-02-27   | 50       |\n",
    "| 5            | 2020-03-01   | 50       |\n",
    "\n",
    "####Output: \n",
    "| product_name       | unit    |\n",
    "|--------------------|---------|\n",
    "| Leetcode Solutions | 130     |\n",
    "| Leetcode Kit       | 100     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f609fcc2-1d94-4945-a32e-957279a370f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_name</th><th>unit</th></tr></thead><tbody><tr><td>Leetcode Solutions</td><td>130</td></tr><tr><td>Leetcode Kit</td><td>100</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Leetcode Solutions",
         130
        ],
        [
         "Leetcode Kit",
         100
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "unit",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month, sum\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data1 = [\n",
    "    (1, 'Leetcode Solutions', 'Book'),\n",
    "    (2, 'Jewels of Stringology', 'Book'),\n",
    "    (3, 'HP', 'Laptop'),\n",
    "    (4, 'Lenovo', 'Laptop'),\n",
    "    (5, 'Leetcode Kit', 'T-shirt')\n",
    "]\n",
    "schema1 = \"product_id int, product_name string, product_category string\"\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "data2 = [\n",
    "    (1, '2020-02-05', 60),\n",
    "    (1, '2020-02-10', 70),\n",
    "    (2, '2020-01-18', 30),\n",
    "    (2, '2020-02-11', 80),\n",
    "    (3,\t'2020-02-17', 2),\n",
    "    (3, '2020-02-24', 3),\n",
    "    (4,\t'2020-03-01', 20),\n",
    "    (4, '2020-03-04', 30),\n",
    "    (4,\t'2020-03-04', 60),\n",
    "    (5,\t'2020-02-25', 50),\n",
    "    (5,\t'2020-02-27', 50),\n",
    "    (5,\t'2020-03-01', 50)\n",
    "]\n",
    "schema2 = \"product_id int, order_date string, unit int\"\n",
    "df2 = spark.createDataFrame(data=data2, schema=schema2)\n",
    "df2 = df2.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "\n",
    "# solution\n",
    "join_df = df2.join(df1, on=\"product_id\", how=\"inner\")\n",
    "group_df = join_df.filter((year(col(\"order_date\")) == 2020) & (month(col(\"order_date\")) == 2)) \\\n",
    "    .groupBy(col(\"product_id\"), col(\"product_name\")) \\\n",
    "    .agg(sum(col(\"unit\")).alias(\"unit\")) \\\n",
    "    .select(col(\"product_name\"), col(\"unit\"))\n",
    "result_df = group_df.filter(col(\"unit\") >= 100)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4034db36-4416-4cce-9d04-9f6d303028e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Find Users With Valid E-Mails\n",
    "###Q.50 Write a solution to find the users who have valid emails.\n",
    "\n",
    "A valid e-mail has a prefix name and a domain where:\n",
    "\n",
    "- The prefix name is a string that may contain letters (upper or lower case), digits, underscore '_', period '.', and/or dash '-'. The prefix name must start with a letter.\n",
    "- The domain is '@leetcode.com'.\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "####Input: \n",
    "Users table:\n",
    "\n",
    "| user_id | name      | mail                    |\n",
    "|---------|-----------|-------------------------|\n",
    "| 1       | Winston   | winston@leetcode.com    |\n",
    "| 2       | Jonathan  | jonathanisgreat         |\n",
    "| 3       | Annabelle | bella-@leetcode.com     |\n",
    "| 4       | Sally     | sally.come@leetcode.com |\n",
    "| 5       | Marwan    | quarz#2020@leetcode.com |\n",
    "| 6       | David     | david69@gmail.com       |\n",
    "| 7       | Shapiro   | .shapo@leetcode.com     |\n",
    "\n",
    "####Output: \n",
    "| user_id | name      | mail                    |\n",
    "|---------|-----------|-------------------------|\n",
    "| 1       | Winston   | winston@leetcode.com    |\n",
    "| 3       | Annabelle | bella-@leetcode.com     |\n",
    "| 4       | Sally     | sally.come@leetcode.com |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5604c0-009b-427c-93c7-8cb22c94637a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>name</th><th>mail</th></tr></thead><tbody><tr><td>1</td><td>Winston</td><td>winston@leetcode.com</td></tr><tr><td>3</td><td>Annabelle</td><td>bella-@leetcode.com</td></tr><tr><td>4</td><td>Sally</td><td>sally.come@leetcode.com</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Winston",
         "winston@leetcode.com"
        ],
        [
         3,
         "Annabelle",
         "bella-@leetcode.com"
        ],
        [
         4,
         "Sally",
         "sally.come@leetcode.com"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mail",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
    "\n",
    "# input dataframe\n",
    "data = [\n",
    "    (1, 'Winston', 'winston@leetcode.com'), \n",
    "    (2, 'Jonathan', 'jonathanisgreat'),\n",
    "    (3,\t'Annabelle', 'bella-@leetcode.com'),\n",
    "    (4,\t'Sally', 'sally.come@leetcode.com'),\n",
    "    (5,\t'Marwan', 'quarz#2020@leetcode.com'),\n",
    "    (6,\t'David', 'david69@gmail.com'),\n",
    "    (7,\t'Shapiro', '.shapo@leetcode.com')\n",
    "]\n",
    "schema = \"user_id int, name string, mail string\"\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# solution\n",
    "pattern = r'^[A-Za-z][A-Za-z0-9._-]*@leetcode\\.com$'\n",
    "result_df = df.filter(col(\"mail\").rlike(pattern)).orderBy(col(\"user_id\"))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcaa49cd-42d6-4237-ad42-020a26eef1fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a173c152-25c0-4317-9e6c-f1454b7e5097",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LeetCode SQL 50 Problems Solution using PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
